{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# What is PyTorch?\n",
    "\n",
    "Itâ€™s a Python based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "-  Tensorial library that uses the power of GPUs\n",
    "-  A deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "## Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch# <Ctrl> / <Shift> + <Return>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting help in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'sq'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msq\u001b[49m  \u001b[38;5;66;03m# <-- Press <Tab> to autocomplete\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\NYU-DL\\Lib\\site-packages\\torch\\__init__.py:2688\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   2685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[32m   2686\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[34m__name__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2688\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch' has no attribute 'sq'"
     ]
    }
   ],
   "source": [
    "torch.sq  # <-- Press <Tab> to autocomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.BFloat16Tensor\n",
       "torch.BoolTensor\n",
       "torch.ByteTensor\n",
       "torch.CharTensor\n",
       "torch.DoubleTensor\n",
       "torch.FloatTensor\n",
       "torch.HalfTensor\n",
       "torch.IntTensor\n",
       "torch.LongTensor\n",
       "torch.ShortTensor\n",
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What about all `*Tensor`s?\n",
    "# Press <esc> to get out of help\n",
    "torch.*Tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Module()  # <Shift>+<Tab>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m torch.nn.Module(*args, **kwargs) -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing them to be nested in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self) -> None:\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will also have their\n",
       "parameters converted when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool\n",
       "\u001b[31mInit docstring:\u001b[39m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[31mFile:\u001b[39m           c:\\users\\philippe\\.conda\\envs\\nyu-dl\\lib\\site-packages\\torch\\nn\\modules\\module.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     Identity, Linear, Bilinear, Threshold, ReLU, RReLU, Hardtanh, Sigmoid, Hardsigmoid, Tanh, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Annotate your functions / classes!\n",
    "torch.nn.Module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m torch.nn.Module(*args, **kwargs) -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mSource:\u001b[39m        \n",
       "\u001b[38;5;28;01mclass\u001b[39;00m Module:\n",
       "    \u001b[33mr\"\"\"Base class for all neural network modules.\u001b[39m\n",
       "\n",
       "\u001b[33m    Your models should also subclass this class.\u001b[39m\n",
       "\n",
       "\u001b[33m    Modules can also contain other Modules, allowing them to be nested in\u001b[39m\n",
       "\u001b[33m    a tree structure. You can assign the submodules as regular attributes::\u001b[39m\n",
       "\n",
       "\u001b[33m        import torch.nn as nn\u001b[39m\n",
       "\u001b[33m        import torch.nn.functional as F\u001b[39m\n",
       "\n",
       "\u001b[33m        class Model(nn.Module):\u001b[39m\n",
       "\u001b[33m            def __init__(self) -> None:\u001b[39m\n",
       "\u001b[33m                super().__init__()\u001b[39m\n",
       "\u001b[33m                self.conv1 = nn.Conv2d(1, 20, 5)\u001b[39m\n",
       "\u001b[33m                self.conv2 = nn.Conv2d(20, 20, 5)\u001b[39m\n",
       "\n",
       "\u001b[33m            def forward(self, x):\u001b[39m\n",
       "\u001b[33m                x = F.relu(self.conv1(x))\u001b[39m\n",
       "\u001b[33m                return F.relu(self.conv2(x))\u001b[39m\n",
       "\n",
       "\u001b[33m    Submodules assigned in this way will be registered, and will also have their\u001b[39m\n",
       "\u001b[33m    parameters converted when you call :meth:`to`, etc.\u001b[39m\n",
       "\n",
       "\u001b[33m    .. note::\u001b[39m\n",
       "\u001b[33m        As per the example above, an ``__init__()`` call to the parent class\u001b[39m\n",
       "\u001b[33m        must be made before assignment on the child.\u001b[39m\n",
       "\n",
       "\u001b[33m    :ivar training: Boolean represents whether this module is in training or\u001b[39m\n",
       "\u001b[33m                    evaluation mode.\u001b[39m\n",
       "\u001b[33m    :vartype training: bool\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "\n",
       "    dump_patches: bool = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
       "\n",
       "    _version: int = \u001b[32m1\u001b[39m\n",
       "    \u001b[33mr\"\"\"This allows better BC support for :meth:`load_state_dict`. In\u001b[39m\n",
       "\u001b[33m    :meth:`state_dict`, the version number will be saved as in the attribute\u001b[39m\n",
       "\u001b[33m    `_metadata` of the returned state dict, and thus pickled. `_metadata` is a\u001b[39m\n",
       "\u001b[33m    dictionary with keys that follow the naming convention of state dict. See\u001b[39m\n",
       "\u001b[33m    ``_load_from_state_dict`` on how to use this information in loading.\u001b[39m\n",
       "\n",
       "\u001b[33m    If new parameters/buffers are added/removed from a module, this number shall\u001b[39m\n",
       "\u001b[33m    be bumped, and the module's `_load_from_state_dict` method can compare the\u001b[39m\n",
       "\u001b[33m    version number and do appropriate changes if the state dict is from before\u001b[39m\n",
       "\u001b[33m    the change.\"\"\"\u001b[39m\n",
       "\n",
       "    training: bool\n",
       "    _parameters: dict[str, Optional[Parameter]]\n",
       "    _buffers: dict[str, Optional[Tensor]]\n",
       "    _non_persistent_buffers_set: set[str]\n",
       "    _backward_pre_hooks: dict[int, Callable]\n",
       "    _backward_hooks: dict[int, Callable]\n",
       "    _is_full_backward_hook: Optional[bool]\n",
       "    _forward_hooks: dict[int, Callable]\n",
       "    \u001b[38;5;66;03m# Marks whether the corresponding _forward_hooks accept kwargs or not.\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# As JIT does not support set[int], this dict is used as a set, where all\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# hooks represented in this dict accept kwargs.\u001b[39;00m\n",
       "    _forward_hooks_with_kwargs: dict[int, bool]\n",
       "    \u001b[38;5;66;03m# forward hooks that should always be called even if an exception is raised\u001b[39;00m\n",
       "    _forward_hooks_always_called: dict[int, bool]\n",
       "    _forward_pre_hooks: dict[int, Callable]\n",
       "    \u001b[38;5;66;03m# Marks whether the corresponding _forward_hooks accept kwargs or not.\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# As JIT does not support set[int], this dict is used as a set, where all\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# hooks represented in this dict accept kwargs.\u001b[39;00m\n",
       "    _forward_pre_hooks_with_kwargs: dict[int, bool]\n",
       "    _state_dict_hooks: dict[int, Callable]\n",
       "    _load_state_dict_pre_hooks: dict[int, Callable]\n",
       "    _state_dict_pre_hooks: dict[int, Callable]\n",
       "    _load_state_dict_post_hooks: dict[int, Callable]\n",
       "    _modules: dict[str, Optional[\u001b[33m\"Module\"\u001b[39m]]\n",
       "    call_super_init: bool = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
       "    _compiled_call_impl: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m __init__(self, *args, **kwargs) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "        \u001b[33m\"\"\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\"\"\"\u001b[39m\n",
       "        torch._C._log_api_usage_once(\u001b[33m\"python.nn_module\"\u001b[39m)\n",
       "\n",
       "        \u001b[38;5;66;03m# Backward compatibility: no args used to be allowed when call_super_init=False\u001b[39;00m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m self.call_super_init \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m bool(kwargs):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
       "                f\"{type(self).__name__}.__init__() got an unexpected keyword argument '{next(iter(kwargs))}'\"\n",
       "                \u001b[33m\"\"\u001b[39m\n",
       "            )\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m self.call_super_init \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m bool(args):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
       "                f\"{type(self).__name__}.__init__() takes 1 positional argument but {len(args) + \u001b[32m1\u001b[39m} were\"\n",
       "                \u001b[33m\" given\"\u001b[39m\n",
       "            )\n",
       "\n",
       "        \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m        Calls super().__setattr__('a', a) instead of the typical self.a = a\u001b[39m\n",
       "\u001b[33m        to avoid Module.__setattr__ overhead. Module's __setattr__ has special\u001b[39m\n",
       "\u001b[33m        handling for parameters, submodules, and buffers but simply calls into\u001b[39m\n",
       "\u001b[33m        super().__setattr__ for all other attributes.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        super().__setattr__(\u001b[33m\"training\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
       "        super().__setattr__(\u001b[33m\"_parameters\"\u001b[39m, {})\n",
       "        super().__setattr__(\u001b[33m\"_buffers\"\u001b[39m, {})\n",
       "        super().__setattr__(\u001b[33m\"_non_persistent_buffers_set\"\u001b[39m, set())\n",
       "        super().__setattr__(\u001b[33m\"_backward_pre_hooks\"\u001b[39m, OrderedDict())\n",
       "        super().__setattr__(\u001b[33m\"_backward_hooks\"\u001b[39m, OrderedDict())\n",
       "        super().__setattr__(\u001b[33m\"_is_full_backward_hook\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
       "        super().__setattr__(\u001b[33m\"_forward_hooks\"\u001b[39m, OrderedDict())\n",
       "        super().__setattr__(\u001b[33m\"_forward_hooks_with_kwargs\"\u001b[39m, OrderedDict())\n",
       "        super().__setattr__(\u001b[33m\"_forward_hooks_always_called\"\u001b[39m, OrderedDict())\n",
       "        super().__setattr__(\u001b[33m\"_forward_pre_hooks\"\u001b[39m, OrderedDict())\n",
       "        super().__setattr__(\u001b[33m\"_forward_pre_hooks_with_kwargs\"\u001b[39m, OrderedDict())\n",
       "        super().__setattr__(\u001b[33m\"_state_dict_hooks\"\u001b[39m, OrderedDict())\n",
       "        super().__setattr__(\u001b[33m\"_state_dict_pre_hooks\"\u001b[39m, OrderedDict())\n",
       "        super().__setattr__(\u001b[33m\"_load_state_dict_pre_hooks\"\u001b[39m, OrderedDict())\n",
       "        super().__setattr__(\u001b[33m\"_load_state_dict_post_hooks\"\u001b[39m, OrderedDict())\n",
       "        super().__setattr__(\u001b[33m\"_modules\"\u001b[39m, {})\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m self.call_super_init:\n",
       "            super().__init__(*args, **kwargs)\n",
       "\n",
       "    forward: Callable[..., Any] = _forward_unimplemented\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m register_buffer(\n",
       "        self, name: str, tensor: Optional[Tensor], persistent: bool = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
       "    ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "        \u001b[33mr\"\"\"Add a buffer to the module.\u001b[39m\n",
       "\n",
       "\u001b[33m        This is typically used to register a buffer that should not to be\u001b[39m\n",
       "\u001b[33m        considered a model parameter. For example, BatchNorm's ``running_mean``\u001b[39m\n",
       "\u001b[33m        is not a parameter, but is part of the module's state. Buffers, by\u001b[39m\n",
       "\u001b[33m        default, are persistent and will be saved alongside parameters. This\u001b[39m\n",
       "\u001b[33m        behavior can be changed by setting :attr:`persistent` to ``False``. The\u001b[39m\n",
       "\u001b[33m        only difference between a persistent buffer and a non-persistent buffer\u001b[39m\n",
       "\u001b[33m        is that the latter will not be a part of this module's\u001b[39m\n",
       "\u001b[33m        :attr:`state_dict`.\u001b[39m\n",
       "\n",
       "\u001b[33m        Buffers can be accessed as attributes using given names.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            name (str): name of the buffer. The buffer can be accessed\u001b[39m\n",
       "\u001b[33m                from this module using the given name\u001b[39m\n",
       "\u001b[33m            tensor (Tensor or None): buffer to be registered. If ``None``, then operations\u001b[39m\n",
       "\u001b[33m                that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\u001b[39m\n",
       "\u001b[33m                the buffer is **not** included in the module's :attr:`state_dict`.\u001b[39m\n",
       "\u001b[33m            persistent (bool): whether the buffer is part of this module's\u001b[39m\n",
       "\u001b[33m                :attr:`state_dict`.\u001b[39m\n",
       "\n",
       "\u001b[33m        Example::\u001b[39m\n",
       "\n",
       "\u001b[33m            >>> # xdoctest: +SKIP(\"undefined vars\")\u001b[39m\n",
       "\u001b[33m            >>> self.register_buffer('running_mean', torch.zeros(num_features))\u001b[39m\n",
       "\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m persistent \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m isinstance(self, torch.jit.ScriptModule):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\u001b[33m\"ScriptModule does not support non-persistent buffers\"\u001b[39m)\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_buffers\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\u001b[33m\"cannot assign buffer before Module.__init__() call\"\u001b[39m)\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(name, str):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
       "                f\"buffer name should be a string. Got {torch.typename(name)}\"\n",
       "            )\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\".\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m name:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m KeyError(\u001b[33m'buffer name can\\'t contain \".\"'\u001b[39m)\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m name == \u001b[33m\"\"\u001b[39m:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m KeyError(\u001b[33m'buffer name can\\'t be empty string \"\"'\u001b[39m)\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m hasattr(self, name) \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._buffers:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m KeyError(f\"attribute '{name}' already exists\")\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m tensor \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(tensor, torch.Tensor):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
       "                f\"cannot assign '{torch.typename(tensor)}' object to buffer '{name}' \"\n",
       "                \u001b[33m\"(torch Tensor or None required)\"\u001b[39m\n",
       "            )\n",
       "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "            \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;28;01min\u001b[39;00m _global_buffer_registration_hooks.values():\n",
       "                output = hook(self, name, tensor)\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                    tensor = output\n",
       "            self._buffers[name] = tensor\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m persistent:\n",
       "                self._non_persistent_buffers_set.discard(name)\n",
       "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                self._non_persistent_buffers_set.add(name)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m register_parameter(self, name: str, param: Optional[Parameter]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "        \u001b[33mr\"\"\"Add a parameter to the module.\u001b[39m\n",
       "\n",
       "\u001b[33m        The parameter can be accessed as an attribute using given name.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            name (str): name of the parameter. The parameter can be accessed\u001b[39m\n",
       "\u001b[33m                from this module using the given name\u001b[39m\n",
       "\u001b[33m            param (Parameter or None): parameter to be added to the module. If\u001b[39m\n",
       "\u001b[33m                ``None``, then operations that run on parameters, such as :attr:`cuda`,\u001b[39m\n",
       "\u001b[33m                are ignored. If ``None``, the parameter is **not** included in the\u001b[39m\n",
       "\u001b[33m                module's :attr:`state_dict`.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_parameters\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\n",
       "                \u001b[33m\"cannot assign parameter before Module.__init__() call\"\u001b[39m\n",
       "            )\n",
       "\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(name, str):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
       "                f\"parameter name should be a string. Got {torch.typename(name)}\"\n",
       "            )\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\".\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m name:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m KeyError(\u001b[33m'parameter name can\\'t contain \".\"'\u001b[39m)\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m name == \u001b[33m\"\"\u001b[39m:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m KeyError(\u001b[33m'parameter name can\\'t be empty string \"\"'\u001b[39m)\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m hasattr(self, name) \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._parameters:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m KeyError(f\"attribute '{name}' already exists\")\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "            self._parameters[name] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(param, Parameter):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
       "                f\"cannot assign '{torch.typename(param)}' object to parameter '{name}' \"\n",
       "                \u001b[33m\"(torch.nn.Parameter or None required)\"\u001b[39m\n",
       "            )\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m param.grad_fn:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\n",
       "                f\"Cannot assign non-leaf Tensor to parameter '{name}'. Model \"\n",
       "                f\"parameters must be created explicitly. To express '{name}' \"\n",
       "                \u001b[33m\"as a function of another Tensor, compute the value in \"\u001b[39m\n",
       "                \u001b[33m\"the forward() method.\"\u001b[39m\n",
       "            )\n",
       "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "            \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;28;01min\u001b[39;00m _global_parameter_registration_hooks.values():\n",
       "                output = hook(self, name, param)\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                    param = output\n",
       "            self._parameters[name] = param\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m add_module(self, name: str, module: Optional[\u001b[33m\"Module\"\u001b[39m]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "        \u001b[33mr\"\"\"Add a child module to the current module.\u001b[39m\n",
       "\n",
       "\u001b[33m        The module can be accessed as an attribute using the given name.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            name (str): name of the child module. The child module can be\u001b[39m\n",
       "\u001b[33m                accessed from this module using the given name\u001b[39m\n",
       "\u001b[33m            module (Module): child module to be added to the module.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(module, Module) \u001b[38;5;28;01mand\u001b[39;00m module \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m TypeError(f\"{torch.typename(module)} is not a Module subclass\")\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(name, str):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
       "                f\"module name should be a string. Got {torch.typename(name)}\"\n",
       "            )\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m hasattr(self, name) \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._modules:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m KeyError(f\"attribute '{name}' already exists\")\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\".\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m name:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m KeyError(f'module name can\\'t contain \".\", got: {name}')\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m name == \u001b[33m\"\"\u001b[39m:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m KeyError(\u001b[33m'module name can\\'t be empty string \"\"'\u001b[39m)\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;28;01min\u001b[39;00m _global_module_registration_hooks.values():\n",
       "            output = hook(self, name, module)\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                module = output\n",
       "        self._modules[name] = module\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m register_module(self, name: str, module: Optional[\u001b[33m\"Module\"\u001b[39m]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "        \u001b[33mr\"\"\"Alias for :func:`add_module`.\"\"\"\u001b[39m\n",
       "        self.add_module(name, module)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m get_submodule(self, target: str) -> \u001b[33m\"Module\"\u001b[39m:\n",
       "        \u001b[33m\"\"\"Return the submodule given by ``target`` if it exists, otherwise throw an error.\u001b[39m\n",
       "\n",
       "\u001b[33m        For example, let's say you have an ``nn.Module`` ``A`` that\u001b[39m\n",
       "\u001b[33m        looks like this:\u001b[39m\n",
       "\n",
       "\u001b[33m        .. code-block:: text\u001b[39m\n",
       "\n",
       "\u001b[33m            A(\u001b[39m\n",
       "\u001b[33m                (net_b): Module(\u001b[39m\n",
       "\u001b[33m                    (net_c): Module(\u001b[39m\n",
       "\u001b[33m                        (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\u001b[39m\n",
       "\u001b[33m                    )\u001b[39m\n",
       "\u001b[33m                    (linear): Linear(in_features=100, out_features=200, bias=True)\u001b[39m\n",
       "\u001b[33m                )\u001b[39m\n",
       "\u001b[33m            )\u001b[39m\n",
       "\n",
       "\u001b[33m        (The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\u001b[39m\n",
       "\u001b[33m        submodule ``net_b``, which itself has two submodules ``net_c``\u001b[39m\n",
       "\u001b[33m        and ``linear``. ``net_c`` then has a submodule ``conv``.)\u001b[39m\n",
       "\n",
       "\u001b[33m        To check whether or not we have the ``linear`` submodule, we\u001b[39m\n",
       "\u001b[33m        would call ``get_submodule(\"net_b.linear\")``. To check whether\u001b[39m\n",
       "\u001b[33m        we have the ``conv`` submodule, we would call\u001b[39m\n",
       "\u001b[33m        ``get_submodule(\"net_b.net_c.conv\")``.\u001b[39m\n",
       "\n",
       "\u001b[33m        The runtime of ``get_submodule`` is bounded by the degree\u001b[39m\n",
       "\u001b[33m        of module nesting in ``target``. A query against\u001b[39m\n",
       "\u001b[33m        ``named_modules`` achieves the same result, but it is O(N) in\u001b[39m\n",
       "\u001b[33m        the number of transitive modules. So, for a simple check to see\u001b[39m\n",
       "\u001b[33m        if some submodule exists, ``get_submodule`` should always be\u001b[39m\n",
       "\u001b[33m        used.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            target: The fully-qualified string name of the submodule\u001b[39m\n",
       "\u001b[33m                to look for. (See above example for how to specify a\u001b[39m\n",
       "\u001b[33m                fully-qualified string.)\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            torch.nn.Module: The submodule referenced by ``target``\u001b[39m\n",
       "\n",
       "\u001b[33m        Raises:\u001b[39m\n",
       "\u001b[33m            AttributeError: If at any point along the path resulting from\u001b[39m\n",
       "\u001b[33m                the target string the (sub)path resolves to a non-existent\u001b[39m\n",
       "\u001b[33m                attribute name or an object that is not an instance of ``nn.Module``.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m target == \u001b[33m\"\"\u001b[39m:\n",
       "            \u001b[38;5;28;01mreturn\u001b[39;00m self\n",
       "\n",
       "        atoms: list[str] = target.split(\u001b[33m\".\"\u001b[39m)\n",
       "        mod: torch.nn.Module = self\n",
       "\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;28;01min\u001b[39;00m atoms:\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m hasattr(mod, item):\n",
       "                \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\n",
       "                    mod._get_name() + \u001b[33m\" has no attribute `\"\u001b[39m + item + \u001b[33m\"`\"\u001b[39m\n",
       "                )\n",
       "\n",
       "            mod = getattr(mod, item)\n",
       "\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(mod, torch.nn.Module):\n",
       "                \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\u001b[33m\"`\"\u001b[39m + item + \u001b[33m\"` is not an nn.Module\"\u001b[39m)\n",
       "\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m mod\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m set_submodule(\n",
       "        self, target: str, module: \u001b[33m\"Module\"\u001b[39m, strict: bool = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
       "    ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "        \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m        Set the submodule given by ``target`` if it exists, otherwise throw an error.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. note::\u001b[39m\n",
       "\u001b[33m            If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\u001b[39m\n",
       "\u001b[33m            or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\u001b[39m\n",
       "\u001b[33m            the method will only attempt to replace an existing submodule and throw an error if\u001b[39m\n",
       "\u001b[33m            the submodule does not exist.\u001b[39m\n",
       "\n",
       "\u001b[33m        For example, let's say you have an ``nn.Module`` ``A`` that\u001b[39m\n",
       "\u001b[33m        looks like this:\u001b[39m\n",
       "\n",
       "\u001b[33m        .. code-block:: text\u001b[39m\n",
       "\n",
       "\u001b[33m            A(\u001b[39m\n",
       "\u001b[33m                (net_b): Module(\u001b[39m\n",
       "\u001b[33m                    (net_c): Module(\u001b[39m\n",
       "\u001b[33m                        (conv): Conv2d(3, 3, 3)\u001b[39m\n",
       "\u001b[33m                    )\u001b[39m\n",
       "\u001b[33m                    (linear): Linear(3, 3)\u001b[39m\n",
       "\u001b[33m                )\u001b[39m\n",
       "\u001b[33m            )\u001b[39m\n",
       "\n",
       "\u001b[33m        (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\u001b[39m\n",
       "\u001b[33m        submodule ``net_b``, which itself has two submodules ``net_c``\u001b[39m\n",
       "\u001b[33m        and ``linear``. ``net_c`` then has a submodule ``conv``.)\u001b[39m\n",
       "\n",
       "\u001b[33m        To override the ``Conv2d`` with a new submodule ``Linear``, you\u001b[39m\n",
       "\u001b[33m        could call ``set_submodule(\"net_b.net_c.conv\", nn.Linear(1, 1))``\u001b[39m\n",
       "\u001b[33m        where ``strict`` could be ``True`` or ``False``\u001b[39m\n",
       "\n",
       "\u001b[33m        To add a new submodule ``Conv2d`` to the existing ``net_b`` module,\u001b[39m\n",
       "\u001b[33m        you would call ``set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1))``.\u001b[39m\n",
       "\n",
       "\u001b[33m        In the above if you set ``strict=True`` and call\u001b[39m\n",
       "\u001b[33m        ``set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1), strict=True)``, an AttributeError\u001b[39m\n",
       "\u001b[33m        will be raised because ``net_b`` does not have a submodule named ``conv``.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            target: The fully-qualified string name of the submodule\u001b[39m\n",
       "\u001b[33m                to look for. (See above example for how to specify a\u001b[39m\n",
       "\u001b[33m                fully-qualified string.)\u001b[39m\n",
       "\u001b[33m            module: The module to set the submodule to.\u001b[39m\n",
       "\u001b[33m            strict: If ``False``, the method will replace an existing submodule\u001b[39m\n",
       "\u001b[33m                or create a new submodule if the parent module exists. If ``True``,\u001b[39m\n",
       "\u001b[33m                the method will only attempt to replace an existing submodule and throw an error\u001b[39m\n",
       "\u001b[33m                if the submodule doesn't already exist.\u001b[39m\n",
       "\n",
       "\u001b[33m        Raises:\u001b[39m\n",
       "\u001b[33m            ValueError: If the ``target`` string is empty or if ``module`` is not an instance of ``nn.Module``.\u001b[39m\n",
       "\u001b[33m            AttributeError: If at any point along the path resulting from\u001b[39m\n",
       "\u001b[33m                the ``target`` string the (sub)path resolves to a non-existent\u001b[39m\n",
       "\u001b[33m                attribute name or an object that is not an instance of ``nn.Module``.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m target == \u001b[33m\"\"\u001b[39m:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m\"Cannot set the submodule without a target name!\"\u001b[39m)\n",
       "\n",
       "        atoms: list[str] = target.split(\u001b[33m\".\"\u001b[39m)\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(module, torch.nn.Module):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\n",
       "                \u001b[33m\"`\"\u001b[39m + \u001b[33m\"module\"\u001b[39m + f\"` is not an nn.Module, found {type(module)}\"\n",
       "            )\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m len(atoms) == \u001b[32m1\u001b[39m:\n",
       "            parent: torch.nn.Module = self\n",
       "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "            parent_key = \u001b[33m\".\"\u001b[39m.join(atoms[:-\u001b[32m1\u001b[39m])\n",
       "            parent = self.get_submodule(parent_key)\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m hasattr(parent, atoms[-\u001b[32m1\u001b[39m]):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\n",
       "                parent._get_name() + \u001b[33m\" has no attribute `\"\u001b[39m + atoms[-\u001b[32m1\u001b[39m] + \u001b[33m\"`\"\u001b[39m\n",
       "            )\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m hasattr(parent, atoms[-\u001b[32m1\u001b[39m]):\n",
       "            mod = getattr(parent, atoms[-\u001b[32m1\u001b[39m])\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(mod, torch.nn.Module):\n",
       "                \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\u001b[33m\"`\"\u001b[39m + atoms[-\u001b[32m1\u001b[39m] + \u001b[33m\"` is not an nn.Module\"\u001b[39m)\n",
       "        setattr(parent, atoms[-\u001b[32m1\u001b[39m], module)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m get_parameter(self, target: str) -> \u001b[33m\"Parameter\"\u001b[39m:\n",
       "        \u001b[33m\"\"\"Return the parameter given by ``target`` if it exists, otherwise throw an error.\u001b[39m\n",
       "\n",
       "\u001b[33m        See the docstring for ``get_submodule`` for a more detailed\u001b[39m\n",
       "\u001b[33m        explanation of this method's functionality as well as how to\u001b[39m\n",
       "\u001b[33m        correctly specify ``target``.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            target: The fully-qualified string name of the Parameter\u001b[39m\n",
       "\u001b[33m                to look for. (See ``get_submodule`` for how to specify a\u001b[39m\n",
       "\u001b[33m                fully-qualified string.)\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            torch.nn.Parameter: The Parameter referenced by ``target``\u001b[39m\n",
       "\n",
       "\u001b[33m        Raises:\u001b[39m\n",
       "\u001b[33m            AttributeError: If the target string references an invalid\u001b[39m\n",
       "\u001b[33m                path or resolves to something that is not an\u001b[39m\n",
       "\u001b[33m                ``nn.Parameter``\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        module_path, _, param_name = target.rpartition(\u001b[33m\".\"\u001b[39m)\n",
       "\n",
       "        mod: torch.nn.Module = self.get_submodule(module_path)\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m hasattr(mod, param_name):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\n",
       "                mod._get_name() + \u001b[33m\" has no attribute `\"\u001b[39m + param_name + \u001b[33m\"`\"\u001b[39m\n",
       "            )\n",
       "\n",
       "        param: torch.nn.Parameter = getattr(mod, param_name)\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(param, torch.nn.Parameter):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\u001b[33m\"`\"\u001b[39m + param_name + \u001b[33m\"` is not an nn.Parameter\"\u001b[39m)\n",
       "\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m param\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m get_buffer(self, target: str) -> \u001b[33m\"Tensor\"\u001b[39m:\n",
       "        \u001b[33m\"\"\"Return the buffer given by ``target`` if it exists, otherwise throw an error.\u001b[39m\n",
       "\n",
       "\u001b[33m        See the docstring for ``get_submodule`` for a more detailed\u001b[39m\n",
       "\u001b[33m        explanation of this method's functionality as well as how to\u001b[39m\n",
       "\u001b[33m        correctly specify ``target``.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            target: The fully-qualified string name of the buffer\u001b[39m\n",
       "\u001b[33m                to look for. (See ``get_submodule`` for how to specify a\u001b[39m\n",
       "\u001b[33m                fully-qualified string.)\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            torch.Tensor: The buffer referenced by ``target``\u001b[39m\n",
       "\n",
       "\u001b[33m        Raises:\u001b[39m\n",
       "\u001b[33m            AttributeError: If the target string references an invalid\u001b[39m\n",
       "\u001b[33m                path or resolves to something that is not a\u001b[39m\n",
       "\u001b[33m                buffer\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        module_path, _, buffer_name = target.rpartition(\u001b[33m\".\"\u001b[39m)\n",
       "\n",
       "        mod: torch.nn.Module = self.get_submodule(module_path)\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m hasattr(mod, buffer_name):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\n",
       "                mod._get_name() + \u001b[33m\" has no attribute `\"\u001b[39m + buffer_name + \u001b[33m\"`\"\u001b[39m\n",
       "            )\n",
       "\n",
       "        buffer: torch.Tensor = getattr(mod, buffer_name)\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m buffer_name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m mod._buffers:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\u001b[33m\"`\"\u001b[39m + buffer_name + \u001b[33m\"` is not a buffer\"\u001b[39m)\n",
       "\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m buffer\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m get_extra_state(self) -> Any:\n",
       "        \u001b[33m\"\"\"Return any extra state to include in the module's state_dict.\u001b[39m\n",
       "\n",
       "\u001b[33m        Implement this and a corresponding :func:`set_extra_state` for your module\u001b[39m\n",
       "\u001b[33m        if you need to store extra state. This function is called when building the\u001b[39m\n",
       "\u001b[33m        module's `state_dict()`.\u001b[39m\n",
       "\n",
       "\u001b[33m        Note that extra state should be picklable to ensure working serialization\u001b[39m\n",
       "\u001b[33m        of the state_dict. We only provide backwards compatibility guarantees\u001b[39m\n",
       "\u001b[33m        for serializing Tensors; other objects may break backwards compatibility if\u001b[39m\n",
       "\u001b[33m        their serialized pickled form changes.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            object: Any extra state to store in the module's state_dict\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\n",
       "            \u001b[33m\"Reached a code path in Module.get_extra_state() that should never be called. \"\u001b[39m\n",
       "            \u001b[33m\"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\u001b[39m\n",
       "            \u001b[33m\"to report this bug.\"\u001b[39m\n",
       "        )\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m set_extra_state(self, state: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "        \u001b[33m\"\"\"Set extra state contained in the loaded `state_dict`.\u001b[39m\n",
       "\n",
       "\u001b[33m        This function is called from :func:`load_state_dict` to handle any extra state\u001b[39m\n",
       "\u001b[33m        found within the `state_dict`. Implement this function and a corresponding\u001b[39m\n",
       "\u001b[33m        :func:`get_extra_state` for your module if you need to store extra state within its\u001b[39m\n",
       "\u001b[33m        `state_dict`.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            state (dict): Extra state from the `state_dict`\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\n",
       "            \u001b[33m\"Reached a code path in Module.set_extra_state() that should never be called. \"\u001b[39m\n",
       "            \u001b[33m\"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\u001b[39m\n",
       "            \u001b[33m\"to report this bug.\"\u001b[39m\n",
       "        )\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m _apply(self, fn, recurse=\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m recurse:\n",
       "            \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;28;01min\u001b[39;00m self.children():\n",
       "                module._apply(fn)\n",
       "\n",
       "        \u001b[38;5;28;01mdef\u001b[39;00m compute_should_use_set_data(tensor, tensor_applied):\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
       "                \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n",
       "                \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n",
       "                \u001b[38;5;66;03m# and the future behavior is to overwrite the existing tensor. However,\u001b[39;00m\n",
       "                \u001b[38;5;66;03m# changing the current behavior is a BC-breaking change, and we want it\u001b[39;00m\n",
       "                \u001b[38;5;66;03m# to happen in future releases. So for now we introduce the\u001b[39;00m\n",
       "                \u001b[38;5;66;03m# `torch.__future__.get_overwrite_module_params_on_conversion()`\u001b[39;00m\n",
       "                \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n",
       "                \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
       "                \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m torch.__future__.get_overwrite_module_params_on_conversion()\n",
       "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
       "\n",
       "        should_use_swap_tensors = (\n",
       "            torch.__future__.get_swap_module_params_on_conversion()\n",
       "        )\n",
       "\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m key, param \u001b[38;5;28;01min\u001b[39;00m self._parameters.items():\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
       "            \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n",
       "            \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n",
       "            \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n",
       "            \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
       "                param_applied = fn(param)\n",
       "            p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
       "\n",
       "            \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
       "            p_should_use_swap_tensors = (\n",
       "                should_use_swap_tensors \u001b[38;5;28;01mor\u001b[39;00m is_traceable_wrapper_subclass(param_applied)\n",
       "            )\n",
       "\n",
       "            param_grad = param.grad\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m p_should_use_swap_tensors:\n",
       "                \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m param_grad \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                        \u001b[38;5;66;03m# Accessing param.grad makes its at::Tensor's use_count 2, which will prevent swapping.\u001b[39;00m\n",
       "                        \u001b[38;5;66;03m# Decrement use count of the gradient by setting to None\u001b[39;00m\n",
       "                        param.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "                    param_applied = torch.nn.Parameter(\n",
       "                        param_applied, requires_grad=param.requires_grad\n",
       "                    )\n",
       "                    torch.utils.swap_tensors(param, param_applied)\n",
       "                \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m param_grad \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                        param.grad = param_grad\n",
       "                    \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\n",
       "                        f\"_apply(): Couldn't swap {self._get_name()}.{key}\"\n",
       "                    ) \u001b[38;5;28;01mfrom\u001b[39;00m e\n",
       "                out_param = param\n",
       "            \u001b[38;5;28;01melif\u001b[39;00m p_should_use_set_data:\n",
       "                param.data = param_applied\n",
       "                out_param = param\n",
       "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                \u001b[38;5;28;01massert\u001b[39;00m isinstance(param, Parameter)\n",
       "                \u001b[38;5;28;01massert\u001b[39;00m param.is_leaf\n",
       "                out_param = Parameter(param_applied, param.requires_grad)\n",
       "                self._parameters[key] = out_param\n",
       "\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m param_grad \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
       "                    grad_applied = fn(param_grad)\n",
       "                g_should_use_set_data = compute_should_use_set_data(\n",
       "                    param_grad, grad_applied\n",
       "                )\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m p_should_use_swap_tensors:\n",
       "                    grad_applied.requires_grad_(param_grad.requires_grad)\n",
       "                    \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "                        torch.utils.swap_tensors(param_grad, grad_applied)\n",
       "                    \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "                        \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\n",
       "                            f\"_apply(): Couldn't swap {self._get_name()}.{key}.grad\"\n",
       "                        ) \u001b[38;5;28;01mfrom\u001b[39;00m e\n",
       "                    out_param.grad = param_grad\n",
       "                \u001b[38;5;28;01melif\u001b[39;00m g_should_use_set_data:\n",
       "                    \u001b[38;5;28;01massert\u001b[39;00m out_param.grad \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "                    out_param.grad.data = grad_applied\n",
       "                \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                    \u001b[38;5;28;01massert\u001b[39;00m param_grad.is_leaf\n",
       "                    out_param.grad = grad_applied.requires_grad_(\n",
       "                        param_grad.requires_grad\n",
       "                    )\n",
       "\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;28;01min\u001b[39;00m self._buffers.items():\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                self._buffers[key] = fn(buf)\n",
       "\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m apply(self: T, fn: Callable[[\u001b[33m\"Module\"\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m]) -> T:\n",
       "        \u001b[33mr\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39m\n",
       "\n",
       "\u001b[33m        Typical use includes initializing the parameters of a model\u001b[39m\n",
       "\u001b[33m        (see also :ref:`nn-init-doc`).\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            fn (:class:`Module` -> None): function to be applied to each submodule\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\n",
       "\u001b[33m        Example::\u001b[39m\n",
       "\n",
       "\u001b[33m            >>> @torch.no_grad()\u001b[39m\n",
       "\u001b[33m            >>> def init_weights(m):\u001b[39m\n",
       "\u001b[33m            >>>     print(m)\u001b[39m\n",
       "\u001b[33m            >>>     if type(m) == nn.Linear:\u001b[39m\n",
       "\u001b[33m            >>>         m.weight.fill_(1.0)\u001b[39m\n",
       "\u001b[33m            >>>         print(m.weight)\u001b[39m\n",
       "\u001b[33m            >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\u001b[39m\n",
       "\u001b[33m            >>> net.apply(init_weights)\u001b[39m\n",
       "\u001b[33m            Linear(in_features=2, out_features=2, bias=True)\u001b[39m\n",
       "\u001b[33m            Parameter containing:\u001b[39m\n",
       "\u001b[33m            tensor([[1., 1.],\u001b[39m\n",
       "\u001b[33m                    [1., 1.]], requires_grad=True)\u001b[39m\n",
       "\u001b[33m            Linear(in_features=2, out_features=2, bias=True)\u001b[39m\n",
       "\u001b[33m            Parameter containing:\u001b[39m\n",
       "\u001b[33m            tensor([[1., 1.],\u001b[39m\n",
       "\u001b[33m                    [1., 1.]], requires_grad=True)\u001b[39m\n",
       "\u001b[33m            Sequential(\u001b[39m\n",
       "\u001b[33m              (0): Linear(in_features=2, out_features=2, bias=True)\u001b[39m\n",
       "\u001b[33m              (1): Linear(in_features=2, out_features=2, bias=True)\u001b[39m\n",
       "\u001b[33m            )\u001b[39m\n",
       "\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;28;01min\u001b[39;00m self.children():\n",
       "            module.apply(fn)\n",
       "        fn(self)\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m cuda(self: T, device: Optional[Union[int, device]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> T:\n",
       "        \u001b[33mr\"\"\"Move all model parameters and buffers to the GPU.\u001b[39m\n",
       "\n",
       "\u001b[33m        This also makes associated parameters and buffers different objects. So\u001b[39m\n",
       "\u001b[33m        it should be called before constructing the optimizer if the module will\u001b[39m\n",
       "\u001b[33m        live on GPU while being optimized.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. note::\u001b[39m\n",
       "\u001b[33m            This method modifies the module in-place.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            device (int, optional): if specified, all parameters will be\u001b[39m\n",
       "\u001b[33m                copied to that device\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t.cuda(device))\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m ipu(self: T, device: Optional[Union[int, device]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> T:\n",
       "        \u001b[33mr\"\"\"Move all model parameters and buffers to the IPU.\u001b[39m\n",
       "\n",
       "\u001b[33m        This also makes associated parameters and buffers different objects. So\u001b[39m\n",
       "\u001b[33m        it should be called before constructing the optimizer if the module will\u001b[39m\n",
       "\u001b[33m        live on IPU while being optimized.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. note::\u001b[39m\n",
       "\u001b[33m            This method modifies the module in-place.\u001b[39m\n",
       "\n",
       "\u001b[33m        Arguments:\u001b[39m\n",
       "\u001b[33m            device (int, optional): if specified, all parameters will be\u001b[39m\n",
       "\u001b[33m                copied to that device\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t.ipu(device))\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m xpu(self: T, device: Optional[Union[int, device]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> T:\n",
       "        \u001b[33mr\"\"\"Move all model parameters and buffers to the XPU.\u001b[39m\n",
       "\n",
       "\u001b[33m        This also makes associated parameters and buffers different objects. So\u001b[39m\n",
       "\u001b[33m        it should be called before constructing optimizer if the module will\u001b[39m\n",
       "\u001b[33m        live on XPU while being optimized.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. note::\u001b[39m\n",
       "\u001b[33m            This method modifies the module in-place.\u001b[39m\n",
       "\n",
       "\u001b[33m        Arguments:\u001b[39m\n",
       "\u001b[33m            device (int, optional): if specified, all parameters will be\u001b[39m\n",
       "\u001b[33m                copied to that device\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t.xpu(device))\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m mtia(self: T, device: Optional[Union[int, device]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> T:\n",
       "        \u001b[33mr\"\"\"Move all model parameters and buffers to the MTIA.\u001b[39m\n",
       "\n",
       "\u001b[33m        This also makes associated parameters and buffers different objects. So\u001b[39m\n",
       "\u001b[33m        it should be called before constructing the optimizer if the module will\u001b[39m\n",
       "\u001b[33m        live on MTIA while being optimized.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. note::\u001b[39m\n",
       "\u001b[33m            This method modifies the module in-place.\u001b[39m\n",
       "\n",
       "\u001b[33m        Arguments:\u001b[39m\n",
       "\u001b[33m            device (int, optional): if specified, all parameters will be\u001b[39m\n",
       "\u001b[33m                copied to that device\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t.mtia(device))\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m cpu(self: T) -> T:\n",
       "        \u001b[33mr\"\"\"Move all model parameters and buffers to the CPU.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. note::\u001b[39m\n",
       "\u001b[33m            This method modifies the module in-place.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t.cpu())\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m type(self: T, dst_type: Union[dtype, str]) -> T:\n",
       "        \u001b[33mr\"\"\"Casts all parameters and buffers to :attr:`dst_type`.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. note::\u001b[39m\n",
       "\u001b[33m            This method modifies the module in-place.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            dst_type (type or string): the desired type\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t.type(dst_type))\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m float(self: T) -> T:\n",
       "        \u001b[33mr\"\"\"Casts all floating point parameters and buffers to ``float`` datatype.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. note::\u001b[39m\n",
       "\u001b[33m            This method modifies the module in-place.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t.float() \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;28;01melse\u001b[39;00m t)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m double(self: T) -> T:\n",
       "        \u001b[33mr\"\"\"Casts all floating point parameters and buffers to ``double`` datatype.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. note::\u001b[39m\n",
       "\u001b[33m            This method modifies the module in-place.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t.double() \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;28;01melse\u001b[39;00m t)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m half(self: T) -> T:\n",
       "        \u001b[33mr\"\"\"Casts all floating point parameters and buffers to ``half`` datatype.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. note::\u001b[39m\n",
       "\u001b[33m            This method modifies the module in-place.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t.half() \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;28;01melse\u001b[39;00m t)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m bfloat16(self: T) -> T:\n",
       "        \u001b[33mr\"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. note::\u001b[39m\n",
       "\u001b[33m            This method modifies the module in-place.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t.bfloat16() \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;28;01melse\u001b[39;00m t)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m to_empty(\n",
       "        self: T, *, device: Optional[DeviceLikeType], recurse: bool = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
       "    ) -> T:\n",
       "        \u001b[33mr\"\"\"Move the parameters and buffers to the specified device without copying storage.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            device (:class:`torch.device`): The desired device of the parameters\u001b[39m\n",
       "\u001b[33m                and buffers in this module.\u001b[39m\n",
       "\u001b[33m            recurse (bool): Whether parameters and buffers of submodules should\u001b[39m\n",
       "\u001b[33m                be recursively moved to the specified device.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self._apply(\n",
       "            \u001b[38;5;28;01mlambda\u001b[39;00m t: torch.empty_like(t, device=device), recurse=recurse\n",
       "        )\n",
       "\n",
       "    @overload\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m to(\n",
       "        self,\n",
       "        device: Optional[DeviceLikeType] = ...,\n",
       "        dtype: Optional[dtype] = ...,\n",
       "        non_blocking: bool = ...,\n",
       "    ) -> Self:\n",
       "        ...\n",
       "\n",
       "    @overload\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m to(self, dtype: dtype, non_blocking: bool = ...) -> Self:\n",
       "        ...\n",
       "\n",
       "    @overload\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m to(self, tensor: Tensor, non_blocking: bool = ...) -> Self:\n",
       "        ...\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m to(self, *args, **kwargs):\n",
       "        \u001b[33mr\"\"\"Move and/or cast the parameters and buffers.\u001b[39m\n",
       "\n",
       "\u001b[33m        This can be called as\u001b[39m\n",
       "\n",
       "\u001b[33m        .. function:: to(device=None, dtype=None, non_blocking=False)\u001b[39m\n",
       "\u001b[33m           :noindex:\u001b[39m\n",
       "\n",
       "\u001b[33m        .. function:: to(dtype, non_blocking=False)\u001b[39m\n",
       "\u001b[33m           :noindex:\u001b[39m\n",
       "\n",
       "\u001b[33m        .. function:: to(tensor, non_blocking=False)\u001b[39m\n",
       "\u001b[33m           :noindex:\u001b[39m\n",
       "\n",
       "\u001b[33m        .. function:: to(memory_format=torch.channels_last)\u001b[39m\n",
       "\u001b[33m           :noindex:\u001b[39m\n",
       "\n",
       "\u001b[33m        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\u001b[39m\n",
       "\u001b[33m        floating point or complex :attr:`dtype`\\ s. In addition, this method will\u001b[39m\n",
       "\u001b[33m        only cast the floating point or complex parameters and buffers to :attr:`dtype`\u001b[39m\n",
       "\u001b[33m        (if given). The integral parameters and buffers will be moved\u001b[39m\n",
       "\u001b[33m        :attr:`device`, if that is given, but with dtypes unchanged. When\u001b[39m\n",
       "\u001b[33m        :attr:`non_blocking` is set, it tries to convert/move asynchronously\u001b[39m\n",
       "\u001b[33m        with respect to the host if possible, e.g., moving CPU Tensors with\u001b[39m\n",
       "\u001b[33m        pinned memory to CUDA devices.\u001b[39m\n",
       "\n",
       "\u001b[33m        See below for examples.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. note::\u001b[39m\n",
       "\u001b[33m            This method modifies the module in-place.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            device (:class:`torch.device`): the desired device of the parameters\u001b[39m\n",
       "\u001b[33m                and buffers in this module\u001b[39m\n",
       "\u001b[33m            dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\u001b[39m\n",
       "\u001b[33m                the parameters and buffers in this module\u001b[39m\n",
       "\u001b[33m            tensor (torch.Tensor): Tensor whose dtype and device are the desired\u001b[39m\n",
       "\u001b[33m                dtype and device for all parameters and buffers in this module\u001b[39m\n",
       "\u001b[33m            memory_format (:class:`torch.memory_format`): the desired memory\u001b[39m\n",
       "\u001b[33m                format for 4D parameters and buffers in this module (keyword\u001b[39m\n",
       "\u001b[33m                only argument)\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\n",
       "\u001b[33m        Examples::\u001b[39m\n",
       "\n",
       "\u001b[33m            >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\u001b[39m\n",
       "\u001b[33m            >>> linear = nn.Linear(2, 2)\u001b[39m\n",
       "\u001b[33m            >>> linear.weight\u001b[39m\n",
       "\u001b[33m            Parameter containing:\u001b[39m\n",
       "\u001b[33m            tensor([[ 0.1913, -0.3420],\u001b[39m\n",
       "\u001b[33m                    [-0.5113, -0.2325]])\u001b[39m\n",
       "\u001b[33m            >>> linear.to(torch.double)\u001b[39m\n",
       "\u001b[33m            Linear(in_features=2, out_features=2, bias=True)\u001b[39m\n",
       "\u001b[33m            >>> linear.weight\u001b[39m\n",
       "\u001b[33m            Parameter containing:\u001b[39m\n",
       "\u001b[33m            tensor([[ 0.1913, -0.3420],\u001b[39m\n",
       "\u001b[33m                    [-0.5113, -0.2325]], dtype=torch.float64)\u001b[39m\n",
       "\u001b[33m            >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\u001b[39m\n",
       "\u001b[33m            >>> gpu1 = torch.device(\"cuda:1\")\u001b[39m\n",
       "\u001b[33m            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\u001b[39m\n",
       "\u001b[33m            Linear(in_features=2, out_features=2, bias=True)\u001b[39m\n",
       "\u001b[33m            >>> linear.weight\u001b[39m\n",
       "\u001b[33m            Parameter containing:\u001b[39m\n",
       "\u001b[33m            tensor([[ 0.1914, -0.3420],\u001b[39m\n",
       "\u001b[33m                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\u001b[39m\n",
       "\u001b[33m            >>> cpu = torch.device(\"cpu\")\u001b[39m\n",
       "\u001b[33m            >>> linear.to(cpu)\u001b[39m\n",
       "\u001b[33m            Linear(in_features=2, out_features=2, bias=True)\u001b[39m\n",
       "\u001b[33m            >>> linear.weight\u001b[39m\n",
       "\u001b[33m            Parameter containing:\u001b[39m\n",
       "\u001b[33m            tensor([[ 0.1914, -0.3420],\u001b[39m\n",
       "\u001b[33m                    [-0.5112, -0.2324]], dtype=torch.float16)\u001b[39m\n",
       "\n",
       "\u001b[33m            >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\u001b[39m\n",
       "\u001b[33m            >>> linear.weight\u001b[39m\n",
       "\u001b[33m            Parameter containing:\u001b[39m\n",
       "\u001b[33m            tensor([[ 0.3741+0.j,  0.2382+0.j],\u001b[39m\n",
       "\u001b[33m                    [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\u001b[39m\n",
       "\u001b[33m            >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\u001b[39m\n",
       "\u001b[33m            tensor([[0.6122+0.j, 0.1150+0.j],\u001b[39m\n",
       "\u001b[33m                    [0.6122+0.j, 0.1150+0.j],\u001b[39m\n",
       "\u001b[33m                    [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\u001b[39m\n",
       "\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(\n",
       "            *args, **kwargs\n",
       "        )\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m (dtype.is_floating_point \u001b[38;5;28;01mor\u001b[39;00m dtype.is_complex):\n",
       "                \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
       "                    \u001b[33m\"nn.Module.to only accepts floating point or complex \"\u001b[39m\n",
       "                    f\"dtypes, but got desired dtype={dtype}\"\n",
       "                )\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m dtype.is_complex:\n",
       "                warnings.warn(\n",
       "                    \u001b[33m\"Complex modules are a new feature under active development whose design may change, \"\u001b[39m\n",
       "                    \u001b[33m\"and some modules might not work as expected when using complex tensors as parameters or buffers. \"\u001b[39m\n",
       "                    \u001b[33m\"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\u001b[39m\n",
       "                    \u001b[33m\"if a complex module does not work as expected.\"\u001b[39m\n",
       "                )\n",
       "\n",
       "        \u001b[38;5;28;01mdef\u001b[39;00m convert(t):\n",
       "            \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m t.dim() \u001b[38;5;28;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n",
       "                    \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n",
       "                        device,\n",
       "                        dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;28;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "                        non_blocking,\n",
       "                        memory_format=convert_to_format,\n",
       "                    )\n",
       "                \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n",
       "                    device,\n",
       "                    dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;28;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "                    non_blocking,\n",
       "                )\n",
       "            \u001b[38;5;28;01mexcept\u001b[39;00m NotImplementedError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m str(e) == \u001b[33m\"Cannot copy out of meta tensor; no data!\"\u001b[39m:\n",
       "                    \u001b[38;5;28;01mraise\u001b[39;00m NotImplementedError(\n",
       "                        f\"{e} Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \"\n",
       "                        f\"when moving module from meta to a different device.\"\n",
       "                    ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "                \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                    \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self._apply(convert)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m register_full_backward_pre_hook(\n",
       "        self,\n",
       "        hook: Callable[[\u001b[33m\"Module\"\u001b[39m, _grad_t], Union[\u001b[38;5;28;01mNone\u001b[39;00m, _grad_t]],\n",
       "        prepend: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    ) -> RemovableHandle:\n",
       "        \u001b[33mr\"\"\"Register a backward pre-hook on the module.\u001b[39m\n",
       "\n",
       "\u001b[33m        The hook will be called every time the gradients for the module are computed.\u001b[39m\n",
       "\u001b[33m        The hook should have the following signature::\u001b[39m\n",
       "\n",
       "\u001b[33m            hook(module, grad_output) -> tuple[Tensor] or None\u001b[39m\n",
       "\n",
       "\u001b[33m        The :attr:`grad_output` is a tuple. The hook should\u001b[39m\n",
       "\u001b[33m        not modify its arguments, but it can optionally return a new gradient with\u001b[39m\n",
       "\u001b[33m        respect to the output that will be used in place of :attr:`grad_output` in\u001b[39m\n",
       "\u001b[33m        subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\u001b[39m\n",
       "\u001b[33m        all non-Tensor arguments.\u001b[39m\n",
       "\n",
       "\u001b[33m        For technical reasons, when this hook is applied to a Module, its forward function will\u001b[39m\n",
       "\u001b[33m        receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\u001b[39m\n",
       "\u001b[33m        of each Tensor returned by the Module's forward function.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. warning ::\u001b[39m\n",
       "\u001b[33m            Modifying inputs inplace is not allowed when using backward hooks and\u001b[39m\n",
       "\u001b[33m            will raise an error.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            hook (Callable): The user-defined hook to be registered.\u001b[39m\n",
       "\u001b[33m            prepend (bool): If true, the provided ``hook`` will be fired before\u001b[39m\n",
       "\u001b[33m                all existing ``backward_pre`` hooks on this\u001b[39m\n",
       "\u001b[33m                :class:`torch.nn.Module`. Otherwise, the provided\u001b[39m\n",
       "\u001b[33m                ``hook`` will be fired after all existing ``backward_pre`` hooks\u001b[39m\n",
       "\u001b[33m                on this :class:`torch.nn.Module`. Note that global\u001b[39m\n",
       "\u001b[33m                ``backward_pre`` hooks registered with\u001b[39m\n",
       "\u001b[33m                :func:`register_module_full_backward_pre_hook` will fire before\u001b[39m\n",
       "\u001b[33m                all hooks registered by this method.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            :class:`torch.utils.hooks.RemovableHandle`:\u001b[39m\n",
       "\u001b[33m                a handle that can be used to remove the added hook by calling\u001b[39m\n",
       "\u001b[33m                ``handle.remove()``\u001b[39m\n",
       "\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        handle = RemovableHandle(self._backward_pre_hooks)\n",
       "        self._backward_pre_hooks[handle.id] = hook\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m prepend:\n",
       "            self._backward_pre_hooks.move_to_end(handle.id, last=\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m register_backward_hook(\n",
       "        self, hook: Callable[[\u001b[33m\"Module\"\u001b[39m, _grad_t, _grad_t], Union[\u001b[38;5;28;01mNone\u001b[39;00m, _grad_t]]\n",
       "    ) -> RemovableHandle:\n",
       "        \u001b[33mr\"\"\"Register a backward hook on the module.\u001b[39m\n",
       "\n",
       "\u001b[33m        This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\u001b[39m\n",
       "\u001b[33m        the behavior of this function will change in future versions.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            :class:`torch.utils.hooks.RemovableHandle`:\u001b[39m\n",
       "\u001b[33m                a handle that can be used to remove the added hook by calling\u001b[39m\n",
       "\u001b[33m                ``handle.remove()``\u001b[39m\n",
       "\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m self._is_full_backward_hook \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\n",
       "                \u001b[33m\"Cannot use both regular backward hooks and full backward hooks on a \"\u001b[39m\n",
       "                \u001b[33m\"single Module. Please use only one of them.\"\u001b[39m\n",
       "            )\n",
       "\n",
       "        self._is_full_backward_hook = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
       "\n",
       "        handle = RemovableHandle(self._backward_hooks)\n",
       "        self._backward_hooks[handle.id] = hook\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m register_full_backward_hook(\n",
       "        self,\n",
       "        hook: Callable[[\u001b[33m\"Module\"\u001b[39m, _grad_t, _grad_t], Union[\u001b[38;5;28;01mNone\u001b[39;00m, _grad_t]],\n",
       "        prepend: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    ) -> RemovableHandle:\n",
       "        \u001b[33mr\"\"\"Register a backward hook on the module.\u001b[39m\n",
       "\n",
       "\u001b[33m        The hook will be called every time the gradients with respect to a module\u001b[39m\n",
       "\u001b[33m        are computed, i.e. the hook will execute if and only if the gradients with\u001b[39m\n",
       "\u001b[33m        respect to module outputs are computed. The hook should have the following\u001b[39m\n",
       "\u001b[33m        signature::\u001b[39m\n",
       "\n",
       "\u001b[33m            hook(module, grad_input, grad_output) -> tuple(Tensor) or None\u001b[39m\n",
       "\n",
       "\u001b[33m        The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\u001b[39m\n",
       "\u001b[33m        with respect to the inputs and outputs respectively. The hook should\u001b[39m\n",
       "\u001b[33m        not modify its arguments, but it can optionally return a new gradient with\u001b[39m\n",
       "\u001b[33m        respect to the input that will be used in place of :attr:`grad_input` in\u001b[39m\n",
       "\u001b[33m        subsequent computations. :attr:`grad_input` will only correspond to the inputs given\u001b[39m\n",
       "\u001b[33m        as positional arguments and all kwarg arguments are ignored. Entries\u001b[39m\n",
       "\u001b[33m        in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\u001b[39m\n",
       "\u001b[33m        arguments.\u001b[39m\n",
       "\n",
       "\u001b[33m        For technical reasons, when this hook is applied to a Module, its forward function will\u001b[39m\n",
       "\u001b[33m        receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\u001b[39m\n",
       "\u001b[33m        of each Tensor returned by the Module's forward function.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. warning ::\u001b[39m\n",
       "\u001b[33m            Modifying inputs or outputs inplace is not allowed when using backward hooks and\u001b[39m\n",
       "\u001b[33m            will raise an error.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            hook (Callable): The user-defined hook to be registered.\u001b[39m\n",
       "\u001b[33m            prepend (bool): If true, the provided ``hook`` will be fired before\u001b[39m\n",
       "\u001b[33m                all existing ``backward`` hooks on this\u001b[39m\n",
       "\u001b[33m                :class:`torch.nn.Module`. Otherwise, the provided\u001b[39m\n",
       "\u001b[33m                ``hook`` will be fired after all existing ``backward`` hooks on\u001b[39m\n",
       "\u001b[33m                this :class:`torch.nn.Module`. Note that global\u001b[39m\n",
       "\u001b[33m                ``backward`` hooks registered with\u001b[39m\n",
       "\u001b[33m                :func:`register_module_full_backward_hook` will fire before\u001b[39m\n",
       "\u001b[33m                all hooks registered by this method.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            :class:`torch.utils.hooks.RemovableHandle`:\u001b[39m\n",
       "\u001b[33m                a handle that can be used to remove the added hook by calling\u001b[39m\n",
       "\u001b[33m                ``handle.remove()``\u001b[39m\n",
       "\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m self._is_full_backward_hook \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\n",
       "                \u001b[33m\"Cannot use both regular backward hooks and full backward hooks on a \"\u001b[39m\n",
       "                \u001b[33m\"single Module. Please use only one of them.\"\u001b[39m\n",
       "            )\n",
       "\n",
       "        self._is_full_backward_hook = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
       "\n",
       "        handle = RemovableHandle(self._backward_hooks)\n",
       "        self._backward_hooks[handle.id] = hook\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m prepend:\n",
       "            self._backward_hooks.move_to_end(handle.id, last=\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m _get_backward_hooks(self):\n",
       "        \u001b[33mr\"\"\"Return the backward hooks for use in the call function.\u001b[39m\n",
       "\n",
       "\u001b[33m        It returns two lists, one with the full backward hooks and one with the non-full\u001b[39m\n",
       "\u001b[33m        backward hooks.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        full_backward_hooks: list[Callable] = []\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m _global_is_full_backward_hook \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
       "            full_backward_hooks += _global_backward_hooks.values()\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m self._is_full_backward_hook \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
       "            full_backward_hooks += self._backward_hooks.values()\n",
       "\n",
       "        non_full_backward_hooks: list[Callable] = []\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m _global_is_full_backward_hook \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
       "            non_full_backward_hooks += _global_backward_hooks.values()\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m self._is_full_backward_hook \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
       "            non_full_backward_hooks += self._backward_hooks.values()\n",
       "\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m full_backward_hooks, non_full_backward_hooks\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m _get_backward_pre_hooks(self):\n",
       "        backward_pre_hooks: list[Callable] = []\n",
       "        backward_pre_hooks += _global_backward_pre_hooks.values()\n",
       "        backward_pre_hooks += self._backward_pre_hooks.values()\n",
       "\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m backward_pre_hooks\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m _maybe_warn_non_full_backward_hook(self, inputs, result, grad_fn):\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(result, torch.Tensor):\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m (\n",
       "                isinstance(result, tuple)\n",
       "                \u001b[38;5;28;01mand\u001b[39;00m all(isinstance(r, torch.Tensor) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;28;01min\u001b[39;00m result)\n",
       "            ):\n",
       "                warnings.warn(\n",
       "                    \u001b[33m\"Using non-full backward hooks on a Module that does not return a \"\u001b[39m\n",
       "                    \u001b[33m\"single Tensor or a tuple of Tensors is deprecated and will be removed \"\u001b[39m\n",
       "                    \u001b[33m\"in future versions. This hook will be missing some of the grad_output. \"\u001b[39m\n",
       "                    \u001b[33m\"Please use register_full_backward_hook to get the documented behavior.\"\u001b[39m,\n",
       "                    FutureWarning,\n",
       "                    stacklevel=\u001b[32m2\u001b[39m,\n",
       "                )\n",
       "                \u001b[38;5;28;01mreturn\u001b[39;00m\n",
       "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "            result = (result,)\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(inputs, torch.Tensor):\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m (\n",
       "                isinstance(inputs, tuple)\n",
       "                \u001b[38;5;28;01mand\u001b[39;00m all(isinstance(i, torch.Tensor) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;28;01min\u001b[39;00m inputs)\n",
       "            ):\n",
       "                warnings.warn(\n",
       "                    \u001b[33m\"Using non-full backward hooks on a Module that does not take as input a \"\u001b[39m\n",
       "                    \u001b[33m\"single Tensor or a tuple of Tensors is deprecated and will be removed \"\u001b[39m\n",
       "                    \u001b[33m\"in future versions. This hook will be missing some of the grad_input. \"\u001b[39m\n",
       "                    \u001b[33m\"Please use register_full_backward_hook to get the documented behavior.\"\u001b[39m,\n",
       "                    FutureWarning,\n",
       "                    stacklevel=\u001b[32m2\u001b[39m,\n",
       "                )\n",
       "                \u001b[38;5;28;01mreturn\u001b[39;00m\n",
       "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "            inputs = (inputs,)\n",
       "\n",
       "        \u001b[38;5;66;03m# At this point we are sure that inputs and result are tuple of Tensors\u001b[39;00m\n",
       "        out_grad_fn = {r.grad_fn \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;28;01min\u001b[39;00m result \u001b[38;5;28;01mif\u001b[39;00m r.grad_fn \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m len(out_grad_fn) == \u001b[32m0\u001b[39m \u001b[38;5;28;01mor\u001b[39;00m (\n",
       "            len(out_grad_fn) == \u001b[32m1\u001b[39m \u001b[38;5;28;01mand\u001b[39;00m grad_fn \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m out_grad_fn\n",
       "        ):\n",
       "            warnings.warn(\n",
       "                \u001b[33m\"Using a non-full backward hook when outputs are nested in python data structure \"\u001b[39m\n",
       "                \u001b[33m\"is deprecated and will be removed in future versions. This hook will be missing \"\u001b[39m\n",
       "                \u001b[33m\"some grad_output.\"\u001b[39m,\n",
       "                FutureWarning,\n",
       "                stacklevel=\u001b[32m2\u001b[39m,\n",
       "            )\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m len(out_grad_fn) > \u001b[32m1\u001b[39m:\n",
       "            warnings.warn(\n",
       "                \u001b[33m\"Using a non-full backward hook when outputs are generated by different autograd Nodes \"\u001b[39m\n",
       "                \u001b[33m\"is deprecated and will be removed in future versions. This hook will be missing \"\u001b[39m\n",
       "                \u001b[33m\"some grad_output. Please use register_full_backward_hook to get the documented behavior.\"\u001b[39m,\n",
       "                FutureWarning,\n",
       "                stacklevel=\u001b[32m2\u001b[39m,\n",
       "            )\n",
       "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "            \u001b[38;5;66;03m# At this point the grad_output part of the hook will most likely be correct\u001b[39;00m\n",
       "            inputs_grad_fn = {i.grad_fn \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;28;01min\u001b[39;00m inputs \u001b[38;5;28;01mif\u001b[39;00m i.grad_fn \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n",
       "\n",
       "            next_functions = {n[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;28;01min\u001b[39;00m grad_fn.next_functions}\n",
       "\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m inputs_grad_fn != next_functions:\n",
       "                warnings.warn(\n",
       "                    \u001b[33m\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\u001b[39m\n",
       "                    \u001b[33m\"is deprecated and will be removed in future versions. This hook will be missing \"\u001b[39m\n",
       "                    \u001b[33m\"some grad_input. Please use register_full_backward_hook to get the documented \"\u001b[39m\n",
       "                    \u001b[33m\"behavior.\"\u001b[39m,\n",
       "                    FutureWarning,\n",
       "                    stacklevel=\u001b[32m2\u001b[39m,\n",
       "                )\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m register_forward_pre_hook(\n",
       "        self,\n",
       "        hook: Union[\n",
       "            Callable[[T, tuple[Any, ...]], Optional[Any]],\n",
       "            Callable[\n",
       "                [T, tuple[Any, ...], dict[str, Any]],\n",
       "                Optional[tuple[Any, dict[str, Any]]],\n",
       "            ],\n",
       "        ],\n",
       "        *,\n",
       "        prepend: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "        with_kwargs: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    ) -> RemovableHandle:\n",
       "        \u001b[33mr\"\"\"Register a forward pre-hook on the module.\u001b[39m\n",
       "\n",
       "\u001b[33m        The hook will be called every time before :func:`forward` is invoked.\u001b[39m\n",
       "\n",
       "\n",
       "\u001b[33m        If ``with_kwargs`` is false or not specified, the input contains only\u001b[39m\n",
       "\u001b[33m        the positional arguments given to the module. Keyword arguments won't be\u001b[39m\n",
       "\u001b[33m        passed to the hooks and only to the ``forward``. The hook can modify the\u001b[39m\n",
       "\u001b[33m        input. User can either return a tuple or a single modified value in the\u001b[39m\n",
       "\u001b[33m        hook. We will wrap the value into a tuple if a single value is returned\u001b[39m\n",
       "\u001b[33m        (unless that value is already a tuple). The hook should have the\u001b[39m\n",
       "\u001b[33m        following signature::\u001b[39m\n",
       "\n",
       "\u001b[33m            hook(module, args) -> None or modified input\u001b[39m\n",
       "\n",
       "\u001b[33m        If ``with_kwargs`` is true, the forward pre-hook will be passed the\u001b[39m\n",
       "\u001b[33m        kwargs given to the forward function. And if the hook modifies the\u001b[39m\n",
       "\u001b[33m        input, both the args and kwargs should be returned. The hook should have\u001b[39m\n",
       "\u001b[33m        the following signature::\u001b[39m\n",
       "\n",
       "\u001b[33m            hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            hook (Callable): The user defined hook to be registered.\u001b[39m\n",
       "\u001b[33m            prepend (bool): If true, the provided ``hook`` will be fired before\u001b[39m\n",
       "\u001b[33m                all existing ``forward_pre`` hooks on this\u001b[39m\n",
       "\u001b[33m                :class:`torch.nn.Module`. Otherwise, the provided\u001b[39m\n",
       "\u001b[33m                ``hook`` will be fired after all existing ``forward_pre`` hooks\u001b[39m\n",
       "\u001b[33m                on this :class:`torch.nn.Module`. Note that global\u001b[39m\n",
       "\u001b[33m                ``forward_pre`` hooks registered with\u001b[39m\n",
       "\u001b[33m                :func:`register_module_forward_pre_hook` will fire before all\u001b[39m\n",
       "\u001b[33m                hooks registered by this method.\u001b[39m\n",
       "\u001b[33m                Default: ``False``\u001b[39m\n",
       "\u001b[33m            with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\u001b[39m\n",
       "\u001b[33m                given to the forward function.\u001b[39m\n",
       "\u001b[33m                Default: ``False``\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            :class:`torch.utils.hooks.RemovableHandle`:\u001b[39m\n",
       "\u001b[33m                a handle that can be used to remove the added hook by calling\u001b[39m\n",
       "\u001b[33m                ``handle.remove()``\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        handle = RemovableHandle(\n",
       "            self._forward_pre_hooks, extra_dict=self._forward_pre_hooks_with_kwargs\n",
       "        )\n",
       "        self._forward_pre_hooks[handle.id] = hook\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m with_kwargs:\n",
       "            self._forward_pre_hooks_with_kwargs[handle.id] = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m prepend:\n",
       "            self._forward_pre_hooks.move_to_end(handle.id, last=\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m register_forward_hook(\n",
       "        self,\n",
       "        hook: Union[\n",
       "            Callable[[T, tuple[Any, ...], Any], Optional[Any]],\n",
       "            Callable[[T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]],\n",
       "        ],\n",
       "        *,\n",
       "        prepend: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "        with_kwargs: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "        always_call: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    ) -> RemovableHandle:\n",
       "        \u001b[33mr\"\"\"Register a forward hook on the module.\u001b[39m\n",
       "\n",
       "\u001b[33m        The hook will be called every time after :func:`forward` has computed an output.\u001b[39m\n",
       "\n",
       "\u001b[33m        If ``with_kwargs`` is ``False`` or not specified, the input contains only\u001b[39m\n",
       "\u001b[33m        the positional arguments given to the module. Keyword arguments won't be\u001b[39m\n",
       "\u001b[33m        passed to the hooks and only to the ``forward``. The hook can modify the\u001b[39m\n",
       "\u001b[33m        output. It can modify the input inplace but it will not have effect on\u001b[39m\n",
       "\u001b[33m        forward since this is called after :func:`forward` is called. The hook\u001b[39m\n",
       "\u001b[33m        should have the following signature::\u001b[39m\n",
       "\n",
       "\u001b[33m            hook(module, args, output) -> None or modified output\u001b[39m\n",
       "\n",
       "\u001b[33m        If ``with_kwargs`` is ``True``, the forward hook will be passed the\u001b[39m\n",
       "\u001b[33m        ``kwargs`` given to the forward function and be expected to return the\u001b[39m\n",
       "\u001b[33m        output possibly modified. The hook should have the following signature::\u001b[39m\n",
       "\n",
       "\u001b[33m            hook(module, args, kwargs, output) -> None or modified output\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            hook (Callable): The user defined hook to be registered.\u001b[39m\n",
       "\u001b[33m            prepend (bool): If ``True``, the provided ``hook`` will be fired\u001b[39m\n",
       "\u001b[33m                before all existing ``forward`` hooks on this\u001b[39m\n",
       "\u001b[33m                :class:`torch.nn.Module`. Otherwise, the provided\u001b[39m\n",
       "\u001b[33m                ``hook`` will be fired after all existing ``forward`` hooks on\u001b[39m\n",
       "\u001b[33m                this :class:`torch.nn.Module`. Note that global\u001b[39m\n",
       "\u001b[33m                ``forward`` hooks registered with\u001b[39m\n",
       "\u001b[33m                :func:`register_module_forward_hook` will fire before all hooks\u001b[39m\n",
       "\u001b[33m                registered by this method.\u001b[39m\n",
       "\u001b[33m                Default: ``False``\u001b[39m\n",
       "\u001b[33m            with_kwargs (bool): If ``True``, the ``hook`` will be passed the\u001b[39m\n",
       "\u001b[33m                kwargs given to the forward function.\u001b[39m\n",
       "\u001b[33m                Default: ``False``\u001b[39m\n",
       "\u001b[33m            always_call (bool): If ``True`` the ``hook`` will be run regardless of\u001b[39m\n",
       "\u001b[33m                whether an exception is raised while calling the Module.\u001b[39m\n",
       "\u001b[33m                Default: ``False``\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            :class:`torch.utils.hooks.RemovableHandle`:\u001b[39m\n",
       "\u001b[33m                a handle that can be used to remove the added hook by calling\u001b[39m\n",
       "\u001b[33m                ``handle.remove()``\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        handle = RemovableHandle(\n",
       "            self._forward_hooks,\n",
       "            extra_dict=[\n",
       "                self._forward_hooks_with_kwargs,\n",
       "                self._forward_hooks_always_called,\n",
       "            ],\n",
       "        )\n",
       "        self._forward_hooks[handle.id] = hook\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m with_kwargs:\n",
       "            self._forward_hooks_with_kwargs[handle.id] = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m always_call:\n",
       "            self._forward_hooks_always_called[handle.id] = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m prepend:\n",
       "            self._forward_hooks.move_to_end(handle.id, last=\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m _slow_forward(self, *input, **kwargs):\n",
       "        tracing_state = torch._C._get_tracing_state()\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m tracing_state \u001b[38;5;28;01mor\u001b[39;00m isinstance(self.forward, torch._C.ScriptMethod):\n",
       "            \u001b[38;5;28;01mreturn\u001b[39;00m self.forward(*input, **kwargs)\n",
       "        recording_scopes = torch.jit._trace._trace_module_map \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
       "            \u001b[38;5;66;03m# type ignore was added because at this point one knows that\u001b[39;00m\n",
       "            \u001b[38;5;66;03m# torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\u001b[39;00m\n",
       "            name = torch.jit._trace._trace_module_map[self] \u001b[38;5;28;01mif\u001b[39;00m self \u001b[38;5;28;01min\u001b[39;00m torch.jit._trace._trace_module_map \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[index, operator] # noqa: B950\u001b[39;00m\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m name:\n",
       "                tracing_state.push_scope(name)\n",
       "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                recording_scopes = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
       "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "            result = self.forward(*input, **kwargs)\n",
       "        \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
       "                tracing_state.pop_scope()\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m _wrapped_call_impl(self, *args, **kwargs):\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m self._compiled_call_impl \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "            \u001b[38;5;28;01mreturn\u001b[39;00m self._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
       "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "            \u001b[38;5;28;01mreturn\u001b[39;00m self._call_impl(*args, **kwargs)\n",
       "\n",
       "    \u001b[38;5;66;03m# torchrec tests the code consistency with the following code\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m _call_impl(self, *args, **kwargs):\n",
       "        forward_call = (self._slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch._C._get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m self.forward)\n",
       "        \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m (self._backward_hooks \u001b[38;5;28;01mor\u001b[39;00m self._backward_pre_hooks \u001b[38;5;28;01mor\u001b[39;00m self._forward_hooks \u001b[38;5;28;01mor\u001b[39;00m self._forward_pre_hooks\n",
       "                \u001b[38;5;28;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;28;01mor\u001b[39;00m _global_backward_hooks\n",
       "                \u001b[38;5;28;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;28;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
       "            \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n",
       "\n",
       "        result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "        called_always_called_hooks = set()\n",
       "\n",
       "        \u001b[38;5;28;01mdef\u001b[39;00m inner():\n",
       "            \u001b[38;5;28;01mnonlocal\u001b[39;00m result, args, kwargs\n",
       "\n",
       "            full_backward_hooks, non_full_backward_hooks = [], []\n",
       "            backward_pre_hooks = []\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m self._backward_pre_hooks \u001b[38;5;28;01mor\u001b[39;00m _global_backward_pre_hooks:\n",
       "                backward_pre_hooks = self._get_backward_pre_hooks()\n",
       "\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m self._backward_hooks \u001b[38;5;28;01mor\u001b[39;00m _global_backward_hooks:\n",
       "                full_backward_hooks, non_full_backward_hooks = self._get_backward_hooks()\n",
       "\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m _global_forward_pre_hooks \u001b[38;5;28;01mor\u001b[39;00m self._forward_pre_hooks:\n",
       "                \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;28;01min\u001b[39;00m (\n",
       "                    *_global_forward_pre_hooks.items(),\n",
       "                    *self._forward_pre_hooks.items(),\n",
       "                ):\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;28;01min\u001b[39;00m self._forward_pre_hooks_with_kwargs:\n",
       "                        args_kwargs_result = hook(self, args, kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
       "                        \u001b[38;5;28;01mif\u001b[39;00m args_kwargs_result \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                            \u001b[38;5;28;01mif\u001b[39;00m isinstance(args_kwargs_result, tuple) \u001b[38;5;28;01mand\u001b[39;00m len(args_kwargs_result) == \u001b[32m2\u001b[39m:\n",
       "                                args, kwargs = args_kwargs_result\n",
       "                            \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                                \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\n",
       "                                    \u001b[33m\"forward pre-hook must return None or a tuple \"\u001b[39m\n",
       "                                    f\"of (new_args, new_kwargs), but got {args_kwargs_result}.\"\n",
       "                                )\n",
       "                    \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                        args_result = hook(self, args)\n",
       "                        \u001b[38;5;28;01mif\u001b[39;00m args_result \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(args_result, tuple):\n",
       "                                args_result = (args_result,)\n",
       "                            args = args_result\n",
       "\n",
       "            bw_hook = \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m full_backward_hooks \u001b[38;5;28;01mor\u001b[39;00m backward_pre_hooks:\n",
       "                bw_hook = BackwardHook(self, full_backward_hooks, backward_pre_hooks)\n",
       "                args = bw_hook.setup_input_hook(args)\n",
       "\n",
       "            result = forward_call(*args, **kwargs)\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;28;01mor\u001b[39;00m self._forward_hooks:\n",
       "                \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;28;01min\u001b[39;00m (\n",
       "                    *_global_forward_hooks.items(),\n",
       "                    *self._forward_hooks.items(),\n",
       "                ):\n",
       "                    \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;28;01min\u001b[39;00m self._forward_hooks_always_called \u001b[38;5;28;01mor\u001b[39;00m hook_id \u001b[38;5;28;01min\u001b[39;00m _global_forward_hooks_always_called:\n",
       "                        called_always_called_hooks.add(hook_id)\n",
       "\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;28;01min\u001b[39;00m self._forward_hooks_with_kwargs \u001b[38;5;28;01mor\u001b[39;00m hook_id \u001b[38;5;28;01min\u001b[39;00m _global_forward_hooks_with_kwargs:\n",
       "                        hook_result = hook(self, args, kwargs, result)\n",
       "                    \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                        hook_result = hook(self, args, result)\n",
       "\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                        result = hook_result\n",
       "\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m bw_hook:\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(result, (torch.Tensor, tuple)):\n",
       "                    warnings.warn(\u001b[33m\"For backward hooks to be called,\"\u001b[39m\n",
       "                                  \u001b[33m\" module output should be a Tensor or a tuple of Tensors\"\u001b[39m\n",
       "                                  f\" but received {type(result)}\")\n",
       "                result = bw_hook.setup_output_hook(result)\n",
       "\n",
       "            \u001b[38;5;66;03m# Handle the non-full backward hooks\u001b[39;00m\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m non_full_backward_hooks:\n",
       "                var = result\n",
       "                \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(var, torch.Tensor):\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m isinstance(var, dict):\n",
       "                        var = next(v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;28;01min\u001b[39;00m var.values() \u001b[38;5;28;01mif\u001b[39;00m isinstance(v, torch.Tensor))\n",
       "                    \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                        var = var[\u001b[32m0\u001b[39m]\n",
       "                grad_fn = var.grad_fn\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m grad_fn \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                    \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;28;01min\u001b[39;00m non_full_backward_hooks:\n",
       "                        grad_fn.register_hook(_WrappedHook(hook, self))\n",
       "                    self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
       "\n",
       "            \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
       "\n",
       "        \u001b[38;5;66;03m# This is technically not behavior equivalent when compiling, but it's\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# incredibly unlikely we will ever support throwing an exception in NN\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# module, and then catching it here, and then reraising it, and then\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# catching it again, and expecting the resulting frame to be compiled.\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# The reraise here just gunks up our exception handling for no good\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# reason.  Don't try to run the always called hooks in event of\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# exception.\u001b[39;00m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m torch.compiler.is_compiling():\n",
       "            \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n",
       "\n",
       "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "            \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n",
       "        \u001b[38;5;28;01mexcept\u001b[39;00m Exception:\n",
       "            \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n",
       "            \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n",
       "            \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n",
       "            \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;28;01min\u001b[39;00m _global_forward_hooks.items():\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;28;01min\u001b[39;00m _global_forward_hooks_always_called \u001b[38;5;28;01mand\u001b[39;00m hook_id \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m called_always_called_hooks:  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n",
       "                    \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "                        hook_result = hook(self, args, result)  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n",
       "                        \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                            result = hook_result\n",
       "                    \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "                        warnings.warn(\u001b[33m\"global module forward hook with ``always_call=True`` raised an exception \"\u001b[39m\n",
       "                                      f\"that was silenced as another error was raised in forward: {str(e)}\")\n",
       "                        \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
       "\n",
       "            \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;28;01min\u001b[39;00m self._forward_hooks.items():\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;28;01min\u001b[39;00m self._forward_hooks_always_called \u001b[38;5;28;01mand\u001b[39;00m hook_id \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m called_always_called_hooks:  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n",
       "                    \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "                        \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;28;01min\u001b[39;00m self._forward_hooks_with_kwargs:\n",
       "                            hook_result = hook(self, args, kwargs, result)  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n",
       "                        \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                            hook_result = hook(self, args, result)  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n",
       "                        \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                            result = hook_result\n",
       "                    \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "                        warnings.warn(\u001b[33m\"module forward hook with ``always_call=True`` raised an exception \"\u001b[39m\n",
       "                                      f\"that was silenced as another error was raised in forward: {str(e)}\")\n",
       "                        \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
       "            \u001b[38;5;66;03m# raise exception raised in try block\u001b[39;00m\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# fmt: on\u001b[39;00m\n",
       "\n",
       "    __call__: Callable[..., Any] = _wrapped_call_impl\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m __getstate__(self):\n",
       "        state = self.__dict__.copy()\n",
       "        state.pop(\u001b[33m\"_compiled_call_impl\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m state\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m __setstate__(self, state):\n",
       "        self.__dict__.update(state)\n",
       "\n",
       "        \u001b[38;5;66;03m# Support loading old checkpoints that don't have the following attrs:\u001b[39;00m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_forward_pre_hooks\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            self._forward_pre_hooks = OrderedDict()\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_forward_pre_hooks_with_kwargs\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            self._forward_pre_hooks_with_kwargs = OrderedDict()\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_forward_hooks_with_kwargs\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            self._forward_hooks_with_kwargs = OrderedDict()\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_forward_hooks_always_called\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            self._forward_hooks_always_called = OrderedDict()\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_state_dict_hooks\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            self._state_dict_hooks = OrderedDict()\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_state_dict_pre_hooks\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            self._state_dict_pre_hooks = OrderedDict()\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_load_state_dict_pre_hooks\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            self._load_state_dict_pre_hooks = OrderedDict()\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_load_state_dict_post_hooks\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            self._load_state_dict_post_hooks = OrderedDict()\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_non_persistent_buffers_set\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            self._non_persistent_buffers_set = set()\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_is_full_backward_hook\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            self._is_full_backward_hook = \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_backward_pre_hooks\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            self._backward_pre_hooks = OrderedDict()\n",
       "\n",
       "    \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m __getattr__(self, name: str) -> Union[Tensor, \u001b[33m\"Module\"\u001b[39m]:\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_parameters\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            _parameters = self.__dict__[\u001b[33m\"_parameters\"\u001b[39m]\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m _parameters:\n",
       "                \u001b[38;5;28;01mreturn\u001b[39;00m _parameters[name]\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_buffers\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            _buffers = self.__dict__[\u001b[33m\"_buffers\"\u001b[39m]\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m _buffers:\n",
       "                \u001b[38;5;28;01mreturn\u001b[39;00m _buffers[name]\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"_modules\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m self.__dict__:\n",
       "            modules = self.__dict__[\u001b[33m\"_modules\"\u001b[39m]\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m modules:\n",
       "                \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n",
       "        \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\n",
       "            f\"'{type(self).__name__}' object has no attribute '{name}'\"\n",
       "        )\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m __setattr__(self, name: str, value: Union[Tensor, \u001b[33m\"Module\"\u001b[39m]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "        \u001b[38;5;28;01mdef\u001b[39;00m remove_from(*dicts_or_sets):\n",
       "            \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;28;01min\u001b[39;00m dicts_or_sets:\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m d:\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m isinstance(d, dict):\n",
       "                        \u001b[38;5;28;01mdel\u001b[39;00m d[name]\n",
       "                    \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                        d.discard(name)\n",
       "\n",
       "        params = self.__dict__.get(\u001b[33m\"_parameters\"\u001b[39m)\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m isinstance(value, Parameter):\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\n",
       "                    \u001b[33m\"cannot assign parameters before Module.__init__() call\"\u001b[39m\n",
       "                )\n",
       "            remove_from(\n",
       "                self.__dict__,\n",
       "                self._buffers,\n",
       "                self._modules,\n",
       "                self._non_persistent_buffers_set,\n",
       "            )\n",
       "            self.register_parameter(name, value)\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m params \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m params:\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
       "                    f\"cannot assign '{torch.typename(value)}' as parameter '{name}' \"\n",
       "                    \u001b[33m\"(torch.nn.Parameter or None expected)\"\u001b[39m\n",
       "                )\n",
       "            self.register_parameter(name, value)\n",
       "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "            modules = self.__dict__.get(\u001b[33m\"_modules\"\u001b[39m)\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m isinstance(value, Module):\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                    \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\n",
       "                        \u001b[33m\"cannot assign module before Module.__init__() call\"\u001b[39m\n",
       "                    )\n",
       "                remove_from(\n",
       "                    self.__dict__,\n",
       "                    self._parameters,\n",
       "                    self._buffers,\n",
       "                    self._non_persistent_buffers_set,\n",
       "                )\n",
       "                \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;28;01min\u001b[39;00m _global_module_registration_hooks.values():\n",
       "                    output = hook(self, name, value)\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                        value = output\n",
       "                modules[name] = value\n",
       "            \u001b[38;5;28;01melif\u001b[39;00m modules \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m modules:\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                    \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
       "                        f\"cannot assign '{torch.typename(value)}' as child module '{name}' \"\n",
       "                        \u001b[33m\"(torch.nn.Module or None expected)\"\u001b[39m\n",
       "                    )\n",
       "                \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;28;01min\u001b[39;00m _global_module_registration_hooks.values():\n",
       "                    output = hook(self, name, value)\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                        value = output\n",
       "                modules[name] = value\n",
       "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                buffers = self.__dict__.get(\u001b[33m\"_buffers\"\u001b[39m)\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m isinstance(value, Buffer) \u001b[38;5;28;01mor\u001b[39;00m buffers \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m buffers:\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(value, torch.Tensor):\n",
       "                        \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
       "                            f\"cannot assign '{torch.typename(value)}' as buffer '{name}' \"\n",
       "                            \u001b[33m\"(torch.nn.Buffer, torch.Tensor or None expected)\"\u001b[39m\n",
       "                        )\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m isinstance(value, Buffer):\n",
       "                        persistent = value.persistent\n",
       "                    \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                        persistent = name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._non_persistent_buffers_set\n",
       "                    \u001b[38;5;66;03m# === HACK ===\u001b[39;00m\n",
       "                    \u001b[38;5;66;03m# This whole block below should just be:\u001b[39;00m\n",
       "                    \u001b[38;5;66;03m# self.register_buffer(name, value, persistent)\u001b[39;00m\n",
       "\n",
       "                    \u001b[38;5;66;03m# But to support subclasses of nn.Module that (wrongfully) implement a\u001b[39;00m\n",
       "                    \u001b[38;5;66;03m# register_buffer() method that doesn't have the \"persistent\"\u001b[39;00m\n",
       "                    \u001b[38;5;66;03m# argument. Only pass it in if it is accepted otherwise assume\u001b[39;00m\n",
       "                    \u001b[38;5;66;03m# it is always true\u001b[39;00m\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m self.register_buffer \u001b[38;5;28;01mis\u001b[39;00m torch.nn.Module.register_buffer:\n",
       "                        self.register_buffer(name, value, persistent)\n",
       "                    \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                        sign = inspect.signature(self.register_buffer)\n",
       "                        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"persistent\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m sign.parameters:\n",
       "                            self.register_buffer(name, value, persistent)\n",
       "                        \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m persistent:\n",
       "                                \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\n",
       "                                    \u001b[33m\"Registering a non-persistent buffer \"\u001b[39m\n",
       "                                    \u001b[33m\"on a Module subclass that implements \"\u001b[39m\n",
       "                                    \u001b[33m\"register_buffer() without the persistent \"\u001b[39m\n",
       "                                    \u001b[33m\"argument is not allowed.\"\u001b[39m\n",
       "                                )\n",
       "                            \u001b[38;5;66;03m# Assume that the implementation without the argument has the\u001b[39;00m\n",
       "                            \u001b[38;5;66;03m# behavior from before the argument was added: persistent=True\u001b[39;00m\n",
       "                            self.register_buffer(name, value)\n",
       "                    \u001b[38;5;66;03m# === HACK END ===\u001b[39;00m\n",
       "                \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                    super().__setattr__(name, value)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m __delattr__(self, name):\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m self._parameters:\n",
       "            \u001b[38;5;28;01mdel\u001b[39;00m self._parameters[name]\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m self._buffers:\n",
       "            \u001b[38;5;28;01mdel\u001b[39;00m self._buffers[name]\n",
       "            self._non_persistent_buffers_set.discard(name)\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m self._modules:\n",
       "            \u001b[38;5;28;01mdel\u001b[39;00m self._modules[name]\n",
       "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "            super().__delattr__(name)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m _register_state_dict_hook(self, hook):\n",
       "        \u001b[33mr\"\"\"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\u001b[39m\n",
       "\n",
       "\u001b[33m        It should have the following signature::\u001b[39m\n",
       "\u001b[33m            hook(module, state_dict, prefix, local_metadata) -> None or state_dict\u001b[39m\n",
       "\n",
       "\u001b[33m        The registered hooks can modify the ``state_dict`` inplace or return a new one.\u001b[39m\n",
       "\u001b[33m        If a new ``state_dict`` is returned, it will only be respected if it is the root\u001b[39m\n",
       "\u001b[33m        module that :meth:`~nn.Module.state_dict` is called from.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m getattr(hook, \u001b[33m\"_from_public_api\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\n",
       "                \u001b[33m\"Cannot register the same function as the state dict post hook that was \"\u001b[39m\n",
       "                \u001b[33m\"previously registered via register_state_dict_post_hook\"\u001b[39m\n",
       "            )\n",
       "        handle = RemovableHandle(self._state_dict_hooks)\n",
       "        self._state_dict_hooks[handle.id] = hook\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m register_state_dict_post_hook(self, hook):\n",
       "        \u001b[33mr\"\"\"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\u001b[39m\n",
       "\n",
       "\u001b[33m        It should have the following signature::\u001b[39m\n",
       "\u001b[33m            hook(module, state_dict, prefix, local_metadata) -> None\u001b[39m\n",
       "\n",
       "\u001b[33m        The registered hooks can modify the ``state_dict`` inplace.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;66;03m# In _register_state_dict_hook there was a bug described in\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/117437 where the return value\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# was only respected for the root module but not child submodules.\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# We fix this in this public version by only allowing inplace modifications on\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# the state_dict by the hook. However, since hooks registered via both these\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# APIs will be added to `_state_dict_hooks` and the type of `_state_dict_hooks`\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# cannot be changed due to many dependencies on it, we mark a hook\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# as being registered via the public API by setting `_from_public_api` on it.\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# In the implementation of `state_dict`, if the callable does not have this\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# flag, the old behavior of respecting the return value will be preserved\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# for the root module, otherwise, we ensure that the hook returns None.\u001b[39;00m\n",
       "        hook._from_public_api = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
       "        handle = RemovableHandle(self._state_dict_hooks)\n",
       "        self._state_dict_hooks[handle.id] = hook\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m register_state_dict_pre_hook(self, hook):\n",
       "        \u001b[33mr\"\"\"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\u001b[39m\n",
       "\n",
       "\u001b[33m        It should have the following signature::\u001b[39m\n",
       "\u001b[33m            hook(module, prefix, keep_vars) -> None\u001b[39m\n",
       "\n",
       "\u001b[33m        The registered hooks can be used to perform pre-processing before the ``state_dict``\u001b[39m\n",
       "\u001b[33m        call is made.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        handle = RemovableHandle(self._state_dict_pre_hooks)\n",
       "        self._state_dict_pre_hooks[handle.id] = hook\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m _save_to_state_dict(self, destination, prefix, keep_vars):\n",
       "        \u001b[33mr\"\"\"Save module state to the `destination` dictionary.\u001b[39m\n",
       "\n",
       "\u001b[33m        The `destination` dictionary will contain the state\u001b[39m\n",
       "\u001b[33m        of the module, but not its descendants. This is called on every\u001b[39m\n",
       "\u001b[33m        submodule in :meth:`~torch.nn.Module.state_dict`.\u001b[39m\n",
       "\n",
       "\u001b[33m        In rare cases, subclasses can achieve class-specific behavior by\u001b[39m\n",
       "\u001b[33m        overriding this method with custom logic.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            destination (dict): a dict where state will be stored\u001b[39m\n",
       "\u001b[33m            prefix (str): the prefix for parameters and buffers used in this\u001b[39m\n",
       "\u001b[33m                module\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;28;01min\u001b[39;00m self._parameters.items():\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                destination[prefix + name] = param \u001b[38;5;28;01mif\u001b[39;00m keep_vars \u001b[38;5;28;01melse\u001b[39;00m param.detach()\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m name, buf \u001b[38;5;28;01min\u001b[39;00m self._buffers.items():\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._non_persistent_buffers_set:\n",
       "                destination[prefix + name] = buf \u001b[38;5;28;01mif\u001b[39;00m keep_vars \u001b[38;5;28;01melse\u001b[39;00m buf.detach()\n",
       "        extra_state_key = prefix + _EXTRA_STATE_KEY_SUFFIX\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m (\n",
       "            getattr(self.__class__, \u001b[33m\"get_extra_state\"\u001b[39m, Module.get_extra_state)\n",
       "            \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m Module.get_extra_state\n",
       "        ):\n",
       "            destination[extra_state_key] = self.get_extra_state()\n",
       "\n",
       "    \u001b[38;5;66;03m# The user can pass an optional arbitrary mappable object to `state_dict`, in which case `state_dict` returns\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# back that same object. But if they pass nothing, an `OrderedDict` is created and returned.\u001b[39;00m\n",
       "    T_destination = TypeVar(\u001b[33m\"T_destination\"\u001b[39m, bound=dict[str, Any])\n",
       "\n",
       "    @overload\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m state_dict(\n",
       "        self, *, destination: T_destination, prefix: str = ..., keep_vars: bool = ...\n",
       "    ) -> T_destination:\n",
       "        ...\n",
       "\n",
       "    @overload\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m state_dict(self, *, prefix: str = ..., keep_vars: bool = ...) -> dict[str, Any]:\n",
       "        ...\n",
       "\n",
       "    \u001b[38;5;66;03m# TODO: Change `*args` to `*` and remove the corresponding warning in docs when BC allows.\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# Also remove the logic for arg parsing together.\u001b[39;00m\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m state_dict(self, *args, destination=\u001b[38;5;28;01mNone\u001b[39;00m, prefix=\u001b[33m\"\"\u001b[39m, keep_vars=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
       "        \u001b[33mr\"\"\"Return a dictionary containing references to the whole state of the module.\u001b[39m\n",
       "\n",
       "\u001b[33m        Both parameters and persistent buffers (e.g. running averages) are\u001b[39m\n",
       "\u001b[33m        included. Keys are corresponding parameter and buffer names.\u001b[39m\n",
       "\u001b[33m        Parameters and buffers set to ``None`` are not included.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. note::\u001b[39m\n",
       "\u001b[33m            The returned object is a shallow copy. It contains references\u001b[39m\n",
       "\u001b[33m            to the module's parameters and buffers.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. warning::\u001b[39m\n",
       "\u001b[33m            Currently ``state_dict()`` also accepts positional arguments for\u001b[39m\n",
       "\u001b[33m            ``destination``, ``prefix`` and ``keep_vars`` in order. However,\u001b[39m\n",
       "\u001b[33m            this is being deprecated and keyword arguments will be enforced in\u001b[39m\n",
       "\u001b[33m            future releases.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. warning::\u001b[39m\n",
       "\u001b[33m            Please avoid the use of argument ``destination`` as it is not\u001b[39m\n",
       "\u001b[33m            designed for end-users.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            destination (dict, optional): If provided, the state of module will\u001b[39m\n",
       "\u001b[33m                be updated into the dict and the same object is returned.\u001b[39m\n",
       "\u001b[33m                Otherwise, an ``OrderedDict`` will be created and returned.\u001b[39m\n",
       "\u001b[33m                Default: ``None``.\u001b[39m\n",
       "\u001b[33m            prefix (str, optional): a prefix added to parameter and buffer\u001b[39m\n",
       "\u001b[33m                names to compose the keys in state_dict. Default: ``''``.\u001b[39m\n",
       "\u001b[33m            keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\u001b[39m\n",
       "\u001b[33m                returned in the state dict are detached from autograd. If it's\u001b[39m\n",
       "\u001b[33m                set to ``True``, detaching will not be performed.\u001b[39m\n",
       "\u001b[33m                Default: ``False``.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            dict:\u001b[39m\n",
       "\u001b[33m                a dictionary containing a whole state of the module\u001b[39m\n",
       "\n",
       "\u001b[33m        Example::\u001b[39m\n",
       "\n",
       "\u001b[33m            >>> # xdoctest: +SKIP(\"undefined vars\")\u001b[39m\n",
       "\u001b[33m            >>> module.state_dict().keys()\u001b[39m\n",
       "\u001b[33m            ['bias', 'weight']\u001b[39m\n",
       "\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;66;03m# TODO: Remove `args` and the parsing logic when BC allows.\u001b[39;00m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m len(args) > \u001b[32m0\u001b[39m:\n",
       "            \u001b[38;5;66;03m# DeprecationWarning is ignored by default\u001b[39;00m\n",
       "            warnings.warn(\n",
       "                \u001b[33m\"Positional args are being deprecated, use kwargs instead. Refer to \"\u001b[39m\n",
       "                \u001b[33m\"https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict\"\u001b[39m\n",
       "                \u001b[33m\" for details.\"\u001b[39m,\n",
       "                FutureWarning,\n",
       "                stacklevel=\u001b[32m2\u001b[39m,\n",
       "            )\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m destination \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                destination = args[\u001b[32m0\u001b[39m]\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m len(args) > \u001b[32m1\u001b[39m \u001b[38;5;28;01mand\u001b[39;00m prefix == \u001b[33m\"\"\u001b[39m:\n",
       "                prefix = args[\u001b[32m1\u001b[39m]\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m len(args) > \u001b[32m2\u001b[39m \u001b[38;5;28;01mand\u001b[39;00m keep_vars \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
       "                keep_vars = args[\u001b[32m2\u001b[39m]\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m destination \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "            destination = OrderedDict()\n",
       "            destination._metadata = OrderedDict()\n",
       "\n",
       "        local_metadata = dict(version=self._version)\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m hasattr(destination, \u001b[33m\"_metadata\"\u001b[39m):\n",
       "            destination._metadata[prefix[:-\u001b[32m1\u001b[39m]] = local_metadata\n",
       "\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;28;01min\u001b[39;00m self._state_dict_pre_hooks.values():\n",
       "            hook(self, prefix, keep_vars)\n",
       "        self._save_to_state_dict(destination, prefix, keep_vars)\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;28;01min\u001b[39;00m self._modules.items():\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                module.state_dict(\n",
       "                    destination=destination,\n",
       "                    prefix=prefix + name + \u001b[33m\".\"\u001b[39m,\n",
       "                    keep_vars=keep_vars,\n",
       "                )\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;28;01min\u001b[39;00m self._state_dict_hooks.values():\n",
       "            hook_result = hook(self, destination, prefix, local_metadata)\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m getattr(hook, \u001b[33m\"_from_public_api\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                    destination = hook_result\n",
       "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                    \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\u001b[33m\"state_dict post-hook must return None\"\u001b[39m)\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m destination\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m _register_load_state_dict_pre_hook(self, hook, with_module=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
       "        \u001b[33mr\"\"\"See :meth:`~torch.nn.Module.register_load_state_dict_pre_hook` for details.\u001b[39m\n",
       "\n",
       "\u001b[33m        A subtle difference is that if ``with_module`` is set to ``False``, then the\u001b[39m\n",
       "\u001b[33m        hook will not take the ``module`` as the first argument whereas\u001b[39m\n",
       "\u001b[33m        :meth:`~torch.nn.Module.register_load_state_dict_pre_hook` always takes the\u001b[39m\n",
       "\u001b[33m        ``module`` as the first argument.\u001b[39m\n",
       "\n",
       "\u001b[33m        Arguments:\u001b[39m\n",
       "\u001b[33m            hook (Callable): Callable hook that will be invoked before\u001b[39m\n",
       "\u001b[33m                loading the state dict.\u001b[39m\n",
       "\u001b[33m            with_module (bool, optional): Whether or not to pass the module\u001b[39m\n",
       "\u001b[33m                instance to the hook as the first parameter.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        handle = RemovableHandle(self._load_state_dict_pre_hooks)\n",
       "        self._load_state_dict_pre_hooks[handle.id] = _WrappedHook(\n",
       "            hook, self \u001b[38;5;28;01mif\u001b[39;00m with_module \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "        )\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m register_load_state_dict_pre_hook(self, hook):\n",
       "        \u001b[33mr\"\"\"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\u001b[39m\n",
       "\n",
       "\u001b[33m        It should have the following signature::\u001b[39m\n",
       "\u001b[33m            hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\u001b[39m\n",
       "\n",
       "\u001b[33m        Arguments:\u001b[39m\n",
       "\u001b[33m            hook (Callable): Callable hook that will be invoked before\u001b[39m\n",
       "\u001b[33m                loading the state dict.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self._register_load_state_dict_pre_hook(hook, with_module=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m register_load_state_dict_post_hook(self, hook):\n",
       "        \u001b[33mr\"\"\"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\u001b[39m\n",
       "\n",
       "\u001b[33m        It should have the following signature::\u001b[39m\n",
       "\u001b[33m            hook(module, incompatible_keys) -> None\u001b[39m\n",
       "\n",
       "\u001b[33m        The ``module`` argument is the current module that this hook is registered\u001b[39m\n",
       "\u001b[33m        on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\u001b[39m\n",
       "\u001b[33m        of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\u001b[39m\n",
       "\u001b[33m        is a ``list`` of ``str`` containing the missing keys and\u001b[39m\n",
       "\u001b[33m        ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\u001b[39m\n",
       "\n",
       "\u001b[33m        The given incompatible_keys can be modified inplace if needed.\u001b[39m\n",
       "\n",
       "\u001b[33m        Note that the checks performed when calling :func:`load_state_dict` with\u001b[39m\n",
       "\u001b[33m        ``strict=True`` are affected by modifications the hook makes to\u001b[39m\n",
       "\u001b[33m        ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\u001b[39m\n",
       "\u001b[33m        set of keys will result in an error being thrown when ``strict=True``, and\u001b[39m\n",
       "\u001b[33m        clearing out both missing and unexpected keys will avoid an error.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            :class:`torch.utils.hooks.RemovableHandle`:\u001b[39m\n",
       "\u001b[33m                a handle that can be used to remove the added hook by calling\u001b[39m\n",
       "\u001b[33m                ``handle.remove()``\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        handle = RemovableHandle(self._load_state_dict_post_hooks)\n",
       "        self._load_state_dict_post_hooks[handle.id] = hook\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m _load_from_state_dict(\n",
       "        self,\n",
       "        state_dict,\n",
       "        prefix,\n",
       "        local_metadata,\n",
       "        strict,\n",
       "        missing_keys,\n",
       "        unexpected_keys,\n",
       "        error_msgs,\n",
       "    ):\n",
       "        \u001b[33mr\"\"\"Copy parameters and buffers from :attr:`state_dict` into only this module, but not its descendants.\u001b[39m\n",
       "\n",
       "\u001b[33m        This is called on every submodule\u001b[39m\n",
       "\u001b[33m        in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\u001b[39m\n",
       "\u001b[33m        module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\u001b[39m\n",
       "\u001b[33m        For state dicts without metadata, :attr:`local_metadata` is empty.\u001b[39m\n",
       "\u001b[33m        Subclasses can achieve class-specific backward compatible loading using\u001b[39m\n",
       "\u001b[33m        the version number at `local_metadata.get(\"version\", None)`.\u001b[39m\n",
       "\u001b[33m        Additionally, :attr:`local_metadata` can also contain the key\u001b[39m\n",
       "\u001b[33m        `assign_to_params_buffers` that indicates whether keys should be\u001b[39m\n",
       "\u001b[33m        assigned their corresponding tensor in the state_dict.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. note::\u001b[39m\n",
       "\u001b[33m            :attr:`state_dict` is not the same object as the input\u001b[39m\n",
       "\u001b[33m            :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\u001b[39m\n",
       "\u001b[33m            it can be modified.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            state_dict (dict): a dict containing parameters and\u001b[39m\n",
       "\u001b[33m                persistent buffers.\u001b[39m\n",
       "\u001b[33m            prefix (str): the prefix for parameters and buffers used in this\u001b[39m\n",
       "\u001b[33m                module\u001b[39m\n",
       "\u001b[33m            local_metadata (dict): a dict containing the metadata for this module.\u001b[39m\n",
       "\u001b[33m                See\u001b[39m\n",
       "\u001b[33m            strict (bool): whether to strictly enforce that the keys in\u001b[39m\n",
       "\u001b[33m                :attr:`state_dict` with :attr:`prefix` match the names of\u001b[39m\n",
       "\u001b[33m                parameters and buffers in this module\u001b[39m\n",
       "\u001b[33m            missing_keys (list of str): if ``strict=True``, add missing keys to\u001b[39m\n",
       "\u001b[33m                this list\u001b[39m\n",
       "\u001b[33m            unexpected_keys (list of str): if ``strict=True``, add unexpected\u001b[39m\n",
       "\u001b[33m                keys to this list\u001b[39m\n",
       "\u001b[33m            error_msgs (list of str): error messages should be added to this\u001b[39m\n",
       "\u001b[33m                list, and will be reported together in\u001b[39m\n",
       "\u001b[33m                :meth:`~torch.nn.Module.load_state_dict`\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;28;01min\u001b[39;00m self._load_state_dict_pre_hooks.values():\n",
       "            hook(\n",
       "                state_dict,\n",
       "                prefix,\n",
       "                local_metadata,\n",
       "                strict,\n",
       "                missing_keys,\n",
       "                unexpected_keys,\n",
       "                error_msgs,\n",
       "            )\n",
       "\n",
       "        persistent_buffers = {\n",
       "            k: v\n",
       "            \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;28;01min\u001b[39;00m self._buffers.items()\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._non_persistent_buffers_set\n",
       "        }\n",
       "        local_name_params = itertools.chain(\n",
       "            self._parameters.items(), persistent_buffers.items()\n",
       "        )\n",
       "        local_state = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;28;01min\u001b[39;00m local_name_params \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n",
       "        assign_to_params_buffers = local_metadata.get(\u001b[33m\"assign_to_params_buffers\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
       "        use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n",
       "\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;28;01min\u001b[39;00m local_state.items():\n",
       "            key = prefix + name\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01min\u001b[39;00m state_dict:\n",
       "                input_param = state_dict[key]\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m torch.overrides.is_tensor_like(input_param):\n",
       "                    error_msgs.append(\n",
       "                        f'While copying the parameter named \"{key}\", '\n",
       "                        \u001b[33m\"expected torch.Tensor or Tensor-like object from checkpoint but \"\u001b[39m\n",
       "                        f\"received {type(input_param)}\"\n",
       "                    )\n",
       "                    \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
       "\n",
       "                \u001b[38;5;66;03m# This is used to avoid copying uninitialized parameters into\u001b[39;00m\n",
       "                \u001b[38;5;66;03m# non-lazy modules, since they dont have the hook to do the checks\u001b[39;00m\n",
       "                \u001b[38;5;66;03m# in such case, it will error when accessing the .shape attribute.\u001b[39;00m\n",
       "                is_param_lazy = torch.nn.parameter.is_lazy(param)\n",
       "                \u001b[38;5;66;03m# Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\u001b[39;00m\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m (\n",
       "                    \u001b[38;5;28;01mnot\u001b[39;00m is_param_lazy\n",
       "                    \u001b[38;5;28;01mand\u001b[39;00m len(param.shape) == \u001b[32m0\u001b[39m\n",
       "                    \u001b[38;5;28;01mand\u001b[39;00m len(input_param.shape) == \u001b[32m1\u001b[39m\n",
       "                ):\n",
       "                    input_param = input_param[\u001b[32m0\u001b[39m]\n",
       "\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m is_param_lazy \u001b[38;5;28;01mand\u001b[39;00m input_param.shape != param.shape:\n",
       "                    \u001b[38;5;66;03m# local shape should match the one in checkpoint\u001b[39;00m\n",
       "                    error_msgs.append(\n",
       "                        f\"size mismatch for {key}: copying a param with shape {input_param.shape} from checkpoint, \"\n",
       "                        f\"the shape in current model is {param.shape}.\"\n",
       "                    )\n",
       "                    \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
       "\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m (\n",
       "                    param.is_meta\n",
       "                    \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m input_param.is_meta\n",
       "                    \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m assign_to_params_buffers\n",
       "                ):\n",
       "                    warnings.warn(\n",
       "                        f\"for {key}: copying from a non-meta parameter in the checkpoint to a meta \"\n",
       "                        \u001b[33m\"parameter in the current model, which is a no-op. (Did you mean to \"\u001b[39m\n",
       "                        \u001b[33m\"pass `assign=True` to assign items in the state dictionary to their \"\u001b[39m\n",
       "                        \u001b[33m\"corresponding key in the module instead of copying them in place?)\"\u001b[39m\n",
       "                    )\n",
       "\n",
       "                \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "                    \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
       "                        \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors:\n",
       "                            new_input_param = param.module_load(\n",
       "                                input_param, assign=assign_to_params_buffers\n",
       "                            )\n",
       "                            \u001b[38;5;28;01mif\u001b[39;00m id(new_input_param) == id(input_param) \u001b[38;5;28;01mor\u001b[39;00m id(\n",
       "                                new_input_param\n",
       "                            ) == id(param):\n",
       "                                \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\n",
       "                                    \u001b[33m\"module_load returned one of self or other, please .detach() \"\u001b[39m\n",
       "                                    \u001b[33m\"the result if returning one of the inputs in module_load\"\u001b[39m\n",
       "                                )\n",
       "                            \u001b[38;5;28;01mif\u001b[39;00m isinstance(param, torch.nn.Parameter):\n",
       "                                \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(new_input_param, torch.nn.Parameter):\n",
       "                                    new_input_param = torch.nn.Parameter(\n",
       "                                        new_input_param,\n",
       "                                        requires_grad=param.requires_grad,\n",
       "                                    )\n",
       "                                \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                                    new_input_param.requires_grad_(param.requires_grad)\n",
       "                            torch.utils.swap_tensors(param, new_input_param)\n",
       "                            \u001b[38;5;28;01mdel\u001b[39;00m new_input_param\n",
       "                        \u001b[38;5;28;01melif\u001b[39;00m assign_to_params_buffers:\n",
       "                            \u001b[38;5;66;03m# Shape checks are already done above\u001b[39;00m\n",
       "                            \u001b[38;5;28;01mif\u001b[39;00m isinstance(param, torch.nn.Parameter):\n",
       "                                \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(input_param, torch.nn.Parameter):\n",
       "                                    input_param = torch.nn.Parameter(\n",
       "                                        input_param, requires_grad=param.requires_grad\n",
       "                                    )\n",
       "                                \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                                    input_param.requires_grad_(param.requires_grad)\n",
       "                            setattr(self, name, input_param)\n",
       "                        \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                            param.copy_(input_param)\n",
       "                \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
       "                    action = \u001b[33m\"swapping\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"copying\"\u001b[39m\n",
       "                    error_msgs.append(\n",
       "                        f'While {action} the parameter named \"{key}\", '\n",
       "                        f\"whose dimensions in the model are {param.size()} and \"\n",
       "                        f\"whose dimensions in the checkpoint are {input_param.size()}, \"\n",
       "                        f\"an exception occurred : {ex.args}.\"\n",
       "                    )\n",
       "            \u001b[38;5;28;01melif\u001b[39;00m strict:\n",
       "                missing_keys.append(key)\n",
       "\n",
       "        extra_state_key = prefix + _EXTRA_STATE_KEY_SUFFIX\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m (\n",
       "            getattr(self.__class__, \u001b[33m\"set_extra_state\"\u001b[39m, Module.set_extra_state)\n",
       "            \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m Module.set_extra_state\n",
       "        ):\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m extra_state_key \u001b[38;5;28;01min\u001b[39;00m state_dict:\n",
       "                self.set_extra_state(state_dict[extra_state_key])\n",
       "            \u001b[38;5;28;01melif\u001b[39;00m strict:\n",
       "                missing_keys.append(extra_state_key)\n",
       "        \u001b[38;5;28;01melif\u001b[39;00m strict \u001b[38;5;28;01mand\u001b[39;00m (extra_state_key \u001b[38;5;28;01min\u001b[39;00m state_dict):\n",
       "            unexpected_keys.append(extra_state_key)\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
       "            \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;28;01min\u001b[39;00m state_dict.keys():\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m key.startswith(prefix) \u001b[38;5;28;01mand\u001b[39;00m key != extra_state_key:\n",
       "                    input_name = key[len(prefix) :].split(\u001b[33m\".\"\u001b[39m, \u001b[32m1\u001b[39m)\n",
       "                    \u001b[38;5;66;03m# Must be Module if it have attributes\u001b[39;00m\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m len(input_name) > \u001b[32m1\u001b[39m:\n",
       "                        \u001b[38;5;28;01mif\u001b[39;00m input_name[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._modules:\n",
       "                            unexpected_keys.append(key)\n",
       "                    \u001b[38;5;28;01melif\u001b[39;00m input_name[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m local_state:\n",
       "                        unexpected_keys.append(key)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m load_state_dict(\n",
       "        self, state_dict: Mapping[str, Any], strict: bool = \u001b[38;5;28;01mTrue\u001b[39;00m, assign: bool = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
       "    ):\n",
       "        \u001b[33mr\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\u001b[39m\n",
       "\n",
       "\u001b[33m        If :attr:`strict` is ``True``, then\u001b[39m\n",
       "\u001b[33m        the keys of :attr:`state_dict` must exactly match the keys returned\u001b[39m\n",
       "\u001b[33m        by this module's :meth:`~torch.nn.Module.state_dict` function.\u001b[39m\n",
       "\n",
       "\u001b[33m        .. warning::\u001b[39m\n",
       "\u001b[33m            If :attr:`assign` is ``True`` the optimizer must be created after\u001b[39m\n",
       "\u001b[33m            the call to :attr:`load_state_dict` unless\u001b[39m\n",
       "\u001b[33m            :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            state_dict (dict): a dict containing parameters and\u001b[39m\n",
       "\u001b[33m                persistent buffers.\u001b[39m\n",
       "\u001b[33m            strict (bool, optional): whether to strictly enforce that the keys\u001b[39m\n",
       "\u001b[33m                in :attr:`state_dict` match the keys returned by this module's\u001b[39m\n",
       "\u001b[33m                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\u001b[39m\n",
       "\u001b[33m            assign (bool, optional): When set to ``False``, the properties of the tensors\u001b[39m\n",
       "\u001b[33m                in the current module are preserved whereas setting it to ``True`` preserves\u001b[39m\n",
       "\u001b[33m                properties of the Tensors in the state dict. The only\u001b[39m\n",
       "\u001b[33m                exception is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s\u001b[39m\n",
       "\u001b[33m                for which the value from the module is preserved.\u001b[39m\n",
       "\u001b[33m                Default: ``False``\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\u001b[39m\n",
       "\u001b[33m                * **missing_keys** is a list of str containing any keys that are expected\u001b[39m\n",
       "\u001b[33m                    by this module but missing from the provided ``state_dict``.\u001b[39m\n",
       "\u001b[33m                * **unexpected_keys** is a list of str containing the keys that are not\u001b[39m\n",
       "\u001b[33m                    expected by this module but present in the provided ``state_dict``.\u001b[39m\n",
       "\n",
       "\u001b[33m        Note:\u001b[39m\n",
       "\u001b[33m            If a parameter or buffer is registered as ``None`` and its corresponding key\u001b[39m\n",
       "\u001b[33m            exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\u001b[39m\n",
       "\u001b[33m            ``RuntimeError``.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(state_dict, Mapping):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
       "                f\"Expected state_dict to be dict-like, got {type(state_dict)}.\"\n",
       "            )\n",
       "\n",
       "        missing_keys: list[str] = []\n",
       "        unexpected_keys: list[str] = []\n",
       "        error_msgs: list[str] = []\n",
       "\n",
       "        \u001b[38;5;66;03m# copy state_dict so _load_from_state_dict can modify it\u001b[39;00m\n",
       "        metadata = getattr(state_dict, \u001b[33m\"_metadata\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
       "        state_dict = OrderedDict(state_dict)\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "            \u001b[38;5;66;03m# mypy isn't aware that \"_metadata\" exists in state_dict\u001b[39;00m\n",
       "            state_dict._metadata = metadata  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
       "\n",
       "        \u001b[38;5;28;01mdef\u001b[39;00m load(module, local_state_dict, prefix=\u001b[33m\"\"\u001b[39m):\n",
       "            local_metadata = {} \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m metadata.get(prefix[:-\u001b[32m1\u001b[39m], {})\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m assign:\n",
       "                local_metadata[\u001b[33m\"assign_to_params_buffers\"\u001b[39m] = assign\n",
       "            module._load_from_state_dict(\n",
       "                local_state_dict,\n",
       "                prefix,\n",
       "                local_metadata,\n",
       "                \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "                missing_keys,\n",
       "                unexpected_keys,\n",
       "                error_msgs,\n",
       "            )\n",
       "            \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;28;01min\u001b[39;00m module._modules.items():\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                    child_prefix = prefix + name + \u001b[33m\".\"\u001b[39m\n",
       "                    child_state_dict = {\n",
       "                        k: v\n",
       "                        \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;28;01min\u001b[39;00m local_state_dict.items()\n",
       "                        \u001b[38;5;28;01mif\u001b[39;00m k.startswith(child_prefix)\n",
       "                    }\n",
       "                    load(child, child_state_dict, child_prefix)  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n",
       "\n",
       "            \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n",
       "            incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
       "            \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;28;01min\u001b[39;00m module._load_state_dict_post_hooks.values():\n",
       "                out = hook(module, incompatible_keys)\n",
       "                \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
       "                    \u001b[33m\"Hooks registered with ``register_load_state_dict_post_hook`` are not\"\u001b[39m\n",
       "                    \u001b[33m\"expected to return new values, if incompatible_keys need to be modified,\"\u001b[39m\n",
       "                    \u001b[33m\"it should be done inplace.\"\u001b[39m\n",
       "                )\n",
       "\n",
       "        load(self, state_dict)\n",
       "        \u001b[38;5;28;01mdel\u001b[39;00m load\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m len(unexpected_keys) > \u001b[32m0\u001b[39m:\n",
       "                error_msgs.insert(\n",
       "                    \u001b[32m0\u001b[39m,\n",
       "                    \u001b[33m\"Unexpected key(s) in state_dict: {}. \"\u001b[39m.format(\n",
       "                        \u001b[33m\", \"\u001b[39m.join(f'\"{k}\"' \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;28;01min\u001b[39;00m unexpected_keys)\n",
       "                    ),\n",
       "                )\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m len(missing_keys) > \u001b[32m0\u001b[39m:\n",
       "                error_msgs.insert(\n",
       "                    \u001b[32m0\u001b[39m,\n",
       "                    \u001b[33m\"Missing key(s) in state_dict: {}. \"\u001b[39m.format(\n",
       "                        \u001b[33m\", \"\u001b[39m.join(f'\"{k}\"' \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;28;01min\u001b[39;00m missing_keys)\n",
       "                    ),\n",
       "                )\n",
       "\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m len(error_msgs) > \u001b[32m0\u001b[39m:\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\n",
       "                \u001b[33m\"Error(s) in loading state_dict for {}:\\n\\t{}\"\u001b[39m.format(\n",
       "                    self.__class__.__name__, \u001b[33m\"\\n\\t\"\u001b[39m.join(error_msgs)\n",
       "                )\n",
       "            )\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m _named_members(\n",
       "        self, get_members_fn, prefix=\u001b[33m\"\"\u001b[39m, recurse=\u001b[38;5;28;01mTrue\u001b[39;00m, remove_duplicate: bool = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
       "    ):\n",
       "        \u001b[33mr\"\"\"Help yield various names + members of modules.\"\"\"\u001b[39m\n",
       "        memo = set()\n",
       "        modules = (\n",
       "            self.named_modules(prefix=prefix, remove_duplicate=remove_duplicate)\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m recurse\n",
       "            \u001b[38;5;28;01melse\u001b[39;00m [(prefix, self)]\n",
       "        )\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m module_prefix, module \u001b[38;5;28;01min\u001b[39;00m modules:\n",
       "            members = get_members_fn(module)\n",
       "            \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;28;01min\u001b[39;00m members:\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mor\u001b[39;00m v \u001b[38;5;28;01min\u001b[39;00m memo:\n",
       "                    \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m remove_duplicate:\n",
       "                    memo.add(v)\n",
       "                name = module_prefix + (\u001b[33m\".\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module_prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\"\u001b[39m) + k\n",
       "                \u001b[38;5;28;01myield\u001b[39;00m name, v\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m parameters(self, recurse: bool = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Iterator[Parameter]:\n",
       "        \u001b[33mr\"\"\"Return an iterator over module parameters.\u001b[39m\n",
       "\n",
       "\u001b[33m        This is typically passed to an optimizer.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            recurse (bool): if True, then yields parameters of this module\u001b[39m\n",
       "\u001b[33m                and all submodules. Otherwise, yields only parameters that\u001b[39m\n",
       "\u001b[33m                are direct members of this module.\u001b[39m\n",
       "\n",
       "\u001b[33m        Yields:\u001b[39m\n",
       "\u001b[33m            Parameter: module parameter\u001b[39m\n",
       "\n",
       "\u001b[33m        Example::\u001b[39m\n",
       "\n",
       "\u001b[33m            >>> # xdoctest: +SKIP(\"undefined vars\")\u001b[39m\n",
       "\u001b[33m            >>> for param in model.parameters():\u001b[39m\n",
       "\u001b[33m            >>>     print(type(param), param.size())\u001b[39m\n",
       "\u001b[33m            <class 'torch.Tensor'> (20L,)\u001b[39m\n",
       "\u001b[33m            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\u001b[39m\n",
       "\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m _name, param \u001b[38;5;28;01min\u001b[39;00m self.named_parameters(recurse=recurse):\n",
       "            \u001b[38;5;28;01myield\u001b[39;00m param\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m named_parameters(\n",
       "        self, prefix: str = \u001b[33m\"\"\u001b[39m, recurse: bool = \u001b[38;5;28;01mTrue\u001b[39;00m, remove_duplicate: bool = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
       "    ) -> Iterator[tuple[str, Parameter]]:\n",
       "        \u001b[33mr\"\"\"Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            prefix (str): prefix to prepend to all parameter names.\u001b[39m\n",
       "\u001b[33m            recurse (bool): if True, then yields parameters of this module\u001b[39m\n",
       "\u001b[33m                and all submodules. Otherwise, yields only parameters that\u001b[39m\n",
       "\u001b[33m                are direct members of this module.\u001b[39m\n",
       "\u001b[33m            remove_duplicate (bool, optional): whether to remove the duplicated\u001b[39m\n",
       "\u001b[33m                parameters in the result. Defaults to True.\u001b[39m\n",
       "\n",
       "\u001b[33m        Yields:\u001b[39m\n",
       "\u001b[33m            (str, Parameter): Tuple containing the name and parameter\u001b[39m\n",
       "\n",
       "\u001b[33m        Example::\u001b[39m\n",
       "\n",
       "\u001b[33m            >>> # xdoctest: +SKIP(\"undefined vars\")\u001b[39m\n",
       "\u001b[33m            >>> for name, param in self.named_parameters():\u001b[39m\n",
       "\u001b[33m            >>>     if name in ['bias']:\u001b[39m\n",
       "\u001b[33m            >>>         print(param.size())\u001b[39m\n",
       "\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        gen = self._named_members(\n",
       "            \u001b[38;5;28;01mlambda\u001b[39;00m module: module._parameters.items(),\n",
       "            prefix=prefix,\n",
       "            recurse=recurse,\n",
       "            remove_duplicate=remove_duplicate,\n",
       "        )\n",
       "        \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m gen\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m buffers(self, recurse: bool = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Iterator[Tensor]:\n",
       "        \u001b[33mr\"\"\"Return an iterator over module buffers.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            recurse (bool): if True, then yields buffers of this module\u001b[39m\n",
       "\u001b[33m                and all submodules. Otherwise, yields only buffers that\u001b[39m\n",
       "\u001b[33m                are direct members of this module.\u001b[39m\n",
       "\n",
       "\u001b[33m        Yields:\u001b[39m\n",
       "\u001b[33m            torch.Tensor: module buffer\u001b[39m\n",
       "\n",
       "\u001b[33m        Example::\u001b[39m\n",
       "\n",
       "\u001b[33m            >>> # xdoctest: +SKIP(\"undefined vars\")\u001b[39m\n",
       "\u001b[33m            >>> for buf in model.buffers():\u001b[39m\n",
       "\u001b[33m            >>>     print(type(buf), buf.size())\u001b[39m\n",
       "\u001b[33m            <class 'torch.Tensor'> (20L,)\u001b[39m\n",
       "\u001b[33m            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\u001b[39m\n",
       "\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m _, buf \u001b[38;5;28;01min\u001b[39;00m self.named_buffers(recurse=recurse):\n",
       "            \u001b[38;5;28;01myield\u001b[39;00m buf\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m named_buffers(\n",
       "        self, prefix: str = \u001b[33m\"\"\u001b[39m, recurse: bool = \u001b[38;5;28;01mTrue\u001b[39;00m, remove_duplicate: bool = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
       "    ) -> Iterator[tuple[str, Tensor]]:\n",
       "        \u001b[33mr\"\"\"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            prefix (str): prefix to prepend to all buffer names.\u001b[39m\n",
       "\u001b[33m            recurse (bool, optional): if True, then yields buffers of this module\u001b[39m\n",
       "\u001b[33m                and all submodules. Otherwise, yields only buffers that\u001b[39m\n",
       "\u001b[33m                are direct members of this module. Defaults to True.\u001b[39m\n",
       "\u001b[33m            remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\u001b[39m\n",
       "\n",
       "\u001b[33m        Yields:\u001b[39m\n",
       "\u001b[33m            (str, torch.Tensor): Tuple containing the name and buffer\u001b[39m\n",
       "\n",
       "\u001b[33m        Example::\u001b[39m\n",
       "\n",
       "\u001b[33m            >>> # xdoctest: +SKIP(\"undefined vars\")\u001b[39m\n",
       "\u001b[33m            >>> for name, buf in self.named_buffers():\u001b[39m\n",
       "\u001b[33m            >>>     if name in ['running_var']:\u001b[39m\n",
       "\u001b[33m            >>>         print(buf.size())\u001b[39m\n",
       "\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        gen = self._named_members(\n",
       "            \u001b[38;5;28;01mlambda\u001b[39;00m module: module._buffers.items(),\n",
       "            prefix=prefix,\n",
       "            recurse=recurse,\n",
       "            remove_duplicate=remove_duplicate,\n",
       "        )\n",
       "        \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m gen\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m children(self) -> Iterator[\u001b[33m\"Module\"\u001b[39m]:\n",
       "        \u001b[33mr\"\"\"Return an iterator over immediate children modules.\u001b[39m\n",
       "\n",
       "\u001b[33m        Yields:\u001b[39m\n",
       "\u001b[33m            Module: a child module\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m _name, module \u001b[38;5;28;01min\u001b[39;00m self.named_children():\n",
       "            \u001b[38;5;28;01myield\u001b[39;00m module\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m named_children(self) -> Iterator[tuple[str, \u001b[33m\"Module\"\u001b[39m]]:\n",
       "        \u001b[33mr\"\"\"Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\u001b[39m\n",
       "\n",
       "\u001b[33m        Yields:\u001b[39m\n",
       "\u001b[33m            (str, Module): Tuple containing a name and child module\u001b[39m\n",
       "\n",
       "\u001b[33m        Example::\u001b[39m\n",
       "\n",
       "\u001b[33m            >>> # xdoctest: +SKIP(\"undefined vars\")\u001b[39m\n",
       "\u001b[33m            >>> for name, module in model.named_children():\u001b[39m\n",
       "\u001b[33m            >>>     if name in ['conv4', 'conv5']:\u001b[39m\n",
       "\u001b[33m            >>>         print(module)\u001b[39m\n",
       "\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        memo = set()\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;28;01min\u001b[39;00m self._modules.items():\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m module \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m memo:\n",
       "                memo.add(module)\n",
       "                \u001b[38;5;28;01myield\u001b[39;00m name, module\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m modules(self) -> Iterator[\u001b[33m\"Module\"\u001b[39m]:\n",
       "        \u001b[33mr\"\"\"Return an iterator over all modules in the network.\u001b[39m\n",
       "\n",
       "\u001b[33m        Yields:\u001b[39m\n",
       "\u001b[33m            Module: a module in the network\u001b[39m\n",
       "\n",
       "\u001b[33m        Note:\u001b[39m\n",
       "\u001b[33m            Duplicate modules are returned only once. In the following\u001b[39m\n",
       "\u001b[33m            example, ``l`` will be returned only once.\u001b[39m\n",
       "\n",
       "\u001b[33m        Example::\u001b[39m\n",
       "\n",
       "\u001b[33m            >>> l = nn.Linear(2, 2)\u001b[39m\n",
       "\u001b[33m            >>> net = nn.Sequential(l, l)\u001b[39m\n",
       "\u001b[33m            >>> for idx, m in enumerate(net.modules()):\u001b[39m\n",
       "\u001b[33m            ...     print(idx, '->', m)\u001b[39m\n",
       "\n",
       "\u001b[33m            0 -> Sequential(\u001b[39m\n",
       "\u001b[33m              (0): Linear(in_features=2, out_features=2, bias=True)\u001b[39m\n",
       "\u001b[33m              (1): Linear(in_features=2, out_features=2, bias=True)\u001b[39m\n",
       "\u001b[33m            )\u001b[39m\n",
       "\u001b[33m            1 -> Linear(in_features=2, out_features=2, bias=True)\u001b[39m\n",
       "\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;28;01min\u001b[39;00m self.named_modules():\n",
       "            \u001b[38;5;28;01myield\u001b[39;00m module\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m named_modules(\n",
       "        self,\n",
       "        memo: Optional[set[\u001b[33m\"Module\"\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "        prefix: str = \u001b[33m\"\"\u001b[39m,\n",
       "        remove_duplicate: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    ):\n",
       "        \u001b[33mr\"\"\"Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            memo: a memo to store the set of modules already added to the result\u001b[39m\n",
       "\u001b[33m            prefix: a prefix that will be added to the name of the module\u001b[39m\n",
       "\u001b[33m            remove_duplicate: whether to remove the duplicated module instances in the result\u001b[39m\n",
       "\u001b[33m                or not\u001b[39m\n",
       "\n",
       "\u001b[33m        Yields:\u001b[39m\n",
       "\u001b[33m            (str, Module): Tuple of name and module\u001b[39m\n",
       "\n",
       "\u001b[33m        Note:\u001b[39m\n",
       "\u001b[33m            Duplicate modules are returned only once. In the following\u001b[39m\n",
       "\u001b[33m            example, ``l`` will be returned only once.\u001b[39m\n",
       "\n",
       "\u001b[33m        Example::\u001b[39m\n",
       "\n",
       "\u001b[33m            >>> l = nn.Linear(2, 2)\u001b[39m\n",
       "\u001b[33m            >>> net = nn.Sequential(l, l)\u001b[39m\n",
       "\u001b[33m            >>> for idx, m in enumerate(net.named_modules()):\u001b[39m\n",
       "\u001b[33m            ...     print(idx, '->', m)\u001b[39m\n",
       "\n",
       "\u001b[33m            0 -> ('', Sequential(\u001b[39m\n",
       "\u001b[33m              (0): Linear(in_features=2, out_features=2, bias=True)\u001b[39m\n",
       "\u001b[33m              (1): Linear(in_features=2, out_features=2, bias=True)\u001b[39m\n",
       "\u001b[33m            ))\u001b[39m\n",
       "\u001b[33m            1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\u001b[39m\n",
       "\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m memo \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "            memo = set()\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m self \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m memo:\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m remove_duplicate:\n",
       "                memo.add(self)\n",
       "            \u001b[38;5;28;01myield\u001b[39;00m prefix, self\n",
       "            \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;28;01min\u001b[39;00m self._modules.items():\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                    \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
       "                submodule_prefix = prefix + (\u001b[33m\".\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\"\u001b[39m) + name\n",
       "                \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m module.named_modules(\n",
       "                    memo, submodule_prefix, remove_duplicate\n",
       "                )\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m train(self: T, mode: bool = \u001b[38;5;28;01mTrue\u001b[39;00m) -> T:\n",
       "        \u001b[33mr\"\"\"Set the module in training mode.\u001b[39m\n",
       "\n",
       "\u001b[33m        This has an effect only on certain modules. See the documentation of\u001b[39m\n",
       "\u001b[33m        particular modules for details of their behaviors in training/evaluation\u001b[39m\n",
       "\u001b[33m        mode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\u001b[39m\n",
       "\u001b[33m        etc.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            mode (bool): whether to set training mode (``True``) or evaluation\u001b[39m\n",
       "\u001b[33m                         mode (``False``). Default: ``True``.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(mode, bool):\n",
       "            \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m\"training mode is expected to be boolean\"\u001b[39m)\n",
       "        self.training = mode\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;28;01min\u001b[39;00m self.children():\n",
       "            module.train(mode)\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m eval(self: T) -> T:\n",
       "        \u001b[33mr\"\"\"Set the module in evaluation mode.\u001b[39m\n",
       "\n",
       "\u001b[33m        This has an effect only on certain modules. See the documentation of\u001b[39m\n",
       "\u001b[33m        particular modules for details of their behaviors in training/evaluation\u001b[39m\n",
       "\u001b[33m        mode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\u001b[39m\n",
       "\u001b[33m        etc.\u001b[39m\n",
       "\n",
       "\u001b[33m        This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\u001b[39m\n",
       "\n",
       "\u001b[33m        See :ref:`locally-disable-grad-doc` for a comparison between\u001b[39m\n",
       "\u001b[33m        `.eval()` and several similar mechanisms that may be confused with it.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self.train(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m requires_grad_(self: T, requires_grad: bool = \u001b[38;5;28;01mTrue\u001b[39;00m) -> T:\n",
       "        \u001b[33mr\"\"\"Change if autograd should record operations on parameters in this module.\u001b[39m\n",
       "\n",
       "\u001b[33m        This method sets the parameters' :attr:`requires_grad` attributes\u001b[39m\n",
       "\u001b[33m        in-place.\u001b[39m\n",
       "\n",
       "\u001b[33m        This method is helpful for freezing part of the module for finetuning\u001b[39m\n",
       "\u001b[33m        or training parts of a model individually (e.g., GAN training).\u001b[39m\n",
       "\n",
       "\u001b[33m        See :ref:`locally-disable-grad-doc` for a comparison between\u001b[39m\n",
       "\u001b[33m        `.requires_grad_()` and several similar mechanisms that may be confused with it.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            requires_grad (bool): whether autograd should record operations on\u001b[39m\n",
       "\u001b[33m                                  parameters in this module. Default: ``True``.\u001b[39m\n",
       "\n",
       "\u001b[33m        Returns:\u001b[39m\n",
       "\u001b[33m            Module: self\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;28;01min\u001b[39;00m self.parameters():\n",
       "            p.requires_grad_(requires_grad)\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m zero_grad(self, set_to_none: bool = \u001b[38;5;28;01mTrue\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "        \u001b[33mr\"\"\"Reset gradients of all model parameters.\u001b[39m\n",
       "\n",
       "\u001b[33m        See similar function under :class:`torch.optim.Optimizer` for more context.\u001b[39m\n",
       "\n",
       "\u001b[33m        Args:\u001b[39m\n",
       "\u001b[33m            set_to_none (bool): instead of setting to zero, set the grads to None.\u001b[39m\n",
       "\u001b[33m                See :meth:`torch.optim.Optimizer.zero_grad` for details.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m getattr(self, \u001b[33m\"_is_replica\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
       "            warnings.warn(\n",
       "                \u001b[33m\"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \"\u001b[39m\n",
       "                \u001b[33m\"The parameters are copied (in a differentiable manner) from the original module. \"\u001b[39m\n",
       "                \u001b[33m\"This means they are not leaf nodes in autograd and so don't accumulate gradients. \"\u001b[39m\n",
       "                \u001b[33m\"If you need gradients in your forward method, consider using autograd.grad instead.\"\u001b[39m\n",
       "            )\n",
       "\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;28;01min\u001b[39;00m self.parameters():\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n",
       "                    p.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "                \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                    \u001b[38;5;28;01mif\u001b[39;00m p.grad.grad_fn \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "                        p.grad.detach_()\n",
       "                    \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                        p.grad.requires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
       "                    p.grad.zero_()\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m share_memory(self: T) -> T:\n",
       "        \u001b[33mr\"\"\"See :meth:`torch.Tensor.share_memory_`.\"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t.share_memory_())\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m _get_name(self):\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self.__class__.__name__\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m extra_repr(self) -> str:\n",
       "        \u001b[33mr\"\"\"Return the extra representation of the module.\u001b[39m\n",
       "\n",
       "\u001b[33m        To print customized extra information, you should re-implement\u001b[39m\n",
       "\u001b[33m        this method in your own modules. Both single-line and multi-line\u001b[39m\n",
       "\u001b[33m        strings are acceptable.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\"\u001b[39m\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m __repr__(self):\n",
       "        \u001b[38;5;66;03m# We treat the extra repr like the sub-module, one item per line\u001b[39;00m\n",
       "        extra_lines = []\n",
       "        extra_repr = self.extra_repr()\n",
       "        \u001b[38;5;66;03m# empty string will be split into list ['']\u001b[39;00m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m extra_repr:\n",
       "            extra_lines = extra_repr.split(\u001b[33m\"\\n\"\u001b[39m)\n",
       "        child_lines = []\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m key, module \u001b[38;5;28;01min\u001b[39;00m self._modules.items():\n",
       "            mod_str = repr(module)\n",
       "            mod_str = _addindent(mod_str, \u001b[32m2\u001b[39m)\n",
       "            child_lines.append(\u001b[33m\"(\"\u001b[39m + key + \u001b[33m\"): \"\u001b[39m + mod_str)\n",
       "        lines = extra_lines + child_lines\n",
       "\n",
       "        main_str = self._get_name() + \u001b[33m\"(\"\u001b[39m\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m lines:\n",
       "            \u001b[38;5;66;03m# simple one-liner info, which most builtin Modules will use\u001b[39;00m\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m len(extra_lines) == \u001b[32m1\u001b[39m \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m child_lines:\n",
       "                main_str += extra_lines[\u001b[32m0\u001b[39m]\n",
       "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "                main_str += \u001b[33m\"\\n  \"\u001b[39m + \u001b[33m\"\\n  \"\u001b[39m.join(lines) + \u001b[33m\"\\n\"\u001b[39m\n",
       "\n",
       "        main_str += \u001b[33m\")\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m main_str\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m __dir__(self):\n",
       "        module_attrs = dir(self.__class__)\n",
       "        attrs = list(self.__dict__.keys())\n",
       "        parameters = list(self._parameters.keys())\n",
       "        modules = list(self._modules.keys())\n",
       "        buffers = list(self._buffers.keys())\n",
       "        keys = module_attrs + attrs + parameters + modules + buffers\n",
       "\n",
       "        \u001b[38;5;66;03m# Eliminate attrs that are not legal Python variable names\u001b[39;00m\n",
       "        keys = [key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;28;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m key[\u001b[32m0\u001b[39m].isdigit()]\n",
       "\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m sorted(keys)\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m _replicate_for_data_parallel(self):\n",
       "        replica = self.__new__(type(self))\n",
       "        replica.__dict__ = self.__dict__.copy()\n",
       "\n",
       "        \u001b[38;5;66;03m# replicas do not have parameters themselves, the replicas reference the original\u001b[39;00m\n",
       "        \u001b[38;5;66;03m# module.\u001b[39;00m\n",
       "        replica._parameters = {}\n",
       "        replica._buffers = replica._buffers.copy()\n",
       "        replica._modules = replica._modules.copy()\n",
       "        replica._is_replica = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
       "\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m replica\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m compile(self, *args, **kwargs):\n",
       "        \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m        Compile this Module's forward using :func:`torch.compile`.\u001b[39m\n",
       "\n",
       "\u001b[33m        This Module's `__call__` method is compiled and all arguments are passed as-is\u001b[39m\n",
       "\u001b[33m        to :func:`torch.compile`.\u001b[39m\n",
       "\n",
       "\u001b[33m        See :func:`torch.compile` for details on the arguments for this function.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        self._compiled_call_impl = torch.compile(self._call_impl, *args, **kwargs)\n",
       "\u001b[31mFile:\u001b[39m           c:\\users\\philippe\\.conda\\envs\\nyu-dl\\lib\\site-packages\\torch\\nn\\modules\\module.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     Identity, Linear, Bilinear, Threshold, ReLU, RReLU, Hardtanh, Sigmoid, Hardsigmoid, Tanh, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.Module??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping to Bash: magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C sâ€™appelle Windows\n",
      " Le numÃ©ro de sÃ©rie du volume est DCC0-C345\n",
      "\n",
      " RÃ©pertoire de C:\\Users\\philippe\\NYU-DLSP20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fichier introuvable\n"
     ]
    }
   ],
   "source": [
    "# List all the files in the current directory\n",
    "!dir -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# List all the files but with cleaner outputs for readability\n",
    "for f in $(ls *.*); do\n",
    "    echo $(wc -l $f)\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting some general help\n",
    "%magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python native data types\n",
    "\n",
    "Python has many native datatypes. Here are the important ones:\n",
    "\n",
    " - **Booleans** are either `True` or `False`.\n",
    " - **Numbers** can be integers (1 and 2), floats (1.1 and 1.2), fractions (1/2 and 2/3), or even complex numbers.\n",
    " - **Strings** are sequences of Unicode characters, e.g. an html document.\n",
    " - **Lists** are ordered sequences of values.\n",
    " - **Tuples** are ordered, immutable sequences of values.\n",
    " - **Sets** are unordered bags of values.\n",
    " - **Dictionaries** are unordered bags of key-value pairs.\n",
    " \n",
    "See [here](http://www.diveintopython3.net/native-datatypes.html) for a complete overview.\n",
    "\n",
    "### More resources\n",
    "\n",
    " 1. Brief Python introduction [here](https://learnxinyminutes.com/docs/python3/).\n",
    " 2. Full Python tutorial [here](https://docs.python.org/3/tutorial/).\n",
    " 3. A Whirlwind Tour of Python [here](https://github.com/jakevdp/WhirlwindTourOfPython).\n",
    " 4. Python Data Science Handbook [here](https://github.com/jakevdp/PythonDataScienceHandbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a tensor of size 2x3x4\n",
    "t = torch.Tensor(2, 3, 4)\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the size of the tensor\n",
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t size: 2 Ã— 3 Ã— 4\n"
     ]
    }
   ],
   "source": [
    "# t.size() is a classic tuple =>\n",
    "print('t size:', ' \\u00D7 '.join(map(str, t.size())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point in a 24 dimensional space\n",
      "organised in 3 sub-dimensions\n"
     ]
    }
   ],
   "source": [
    "# prints dimensional space and sub-dimensions\n",
    "print(f'point in a {t.numel()} dimensional space')\n",
    "print(f'organised in {t.dim()} sub-dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.1034e-25,  1.9730e-42,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 1., 3., 5.],\n",
       "         [9., 3., 1., 3.],\n",
       "         [1., 2., 3., 1.]],\n",
       "\n",
       "        [[7., 2., 2., 0.],\n",
       "         [2., 4., 5., 9.],\n",
       "         [2., 6., 8., 0.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mind the underscore!\n",
    "# Any operation that mutates a tensor in-place is post-fixed with an _.\n",
    "# For example: x.copy_(y), x.t_(), x.random_(n) will change x.\n",
    "t.random_(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 1., 3., 5.],\n",
       "         [9., 3., 1., 3.],\n",
       "         [1., 2., 3., 1.]],\n",
       "\n",
       "        [[7., 2., 2., 0.],\n",
       "         [2., 4., 5., 9.],\n",
       "         [2., 6., 8., 0.]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1., 3., 5., 9., 3., 1., 3.],\n",
       "        [1., 2., 3., 1., 7., 2., 2., 0.],\n",
       "        [2., 4., 5., 9., 2., 6., 8., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This resizes the tensor permanently \n",
    "r = torch.Tensor(t)\n",
    "r.resize_(3, 8)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As you can see zero_ would replace r with 0's which was originally filled with integers\n",
    "r.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This *is* important, sigh...\n",
    "s = r.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In-place fill of 1's\n",
    "s.fill_(1)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because we cloned r, even though we did an in-place operation, this doesn't affect r\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors (1D Tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a 1D tensor of integers 1 to 4\n",
    "v = torch.Tensor([1, 2, 3, 4])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim: 1, size: 4\n"
     ]
    }
   ],
   "source": [
    "# Print number of dimensions (1D) and size of tensor\n",
    "print(f'dim: {v.dim()}, size: {v.size()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 2., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.Tensor([1, 0, 2, 0])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 6., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Element-wise multiplication\n",
    "v * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar product: 1*1 + 2*0 + 3*2 + 4*0\n",
    "v @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 8., 8., 4., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In-place replacement of random number from 0 to 10\n",
    "x = torch.Tensor(5).random_(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first: 1.0, last: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f'first: {x[0]}, last: {x[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8., 8.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract sub-Tensor [from:to)\n",
    "x[1:2 + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor with integers ranging from 1 to 5, excluding 5\n",
    "v = torch.arange(1, 4 + 1)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  4,  9, 16]) tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Square all elements in the tensor\n",
    "print(v.pow(2), v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices (2D Tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 5., 3., 7.],\n",
       "        [4., 2., 1., 9.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 2x4 tensor\n",
    "m = torch.Tensor([[2, 5, 3, 7],\n",
    "                  [4, 2, 1, 9]])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 -- 4 -- torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(m.size(0), m.size(1), m.size(), sep=' -- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the total number of elements, hence num-el (number of elements)\n",
    "m.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing row 0, column 2 (0-indexed)\n",
    "m[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing row 0, column 2 (0-indexed)\n",
    "m[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 2.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing column 1, all rows (returns size 2)\n",
    "m[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.],\n",
       "        [2.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing column 1, all rows (returns size 2x1)\n",
    "m[:, [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 5., 3., 7.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexes row 0, all columns (returns 1x4)\n",
    "m[[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 5., 3., 7.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexes row 0, all columns (returns size 4)\n",
    "m[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensor of numbers from 1 to 5 (excluding 5)\n",
    "v = torch.arange(1., 4 + 1)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 5., 3., 7.],\n",
       "        [4., 2., 1., 9.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([49., 47.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar product\n",
    "m @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([49.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculated by 1*2 + 2*5 + 3*3 + 4*7\n",
    "m[[0], :] @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculated by \n",
    "m[[1], :] @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3491, 5.7346, 3.0574, 7.5426],\n",
       "        [4.9584, 2.9263, 1.8296, 9.6602]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a random tensor of size 2x4 to m\n",
    "m + torch.rand(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7695, 4.1258, 2.8722, 6.4592],\n",
       "        [3.3371, 1.9942, 0.7839, 8.4851]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtract a random tensor of size 2x4 to m\n",
    "m - torch.rand(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4725, 4.1504, 0.4045, 2.2180],\n",
       "        [2.2619, 0.1198, 0.6409, 0.4191]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply a random tensor of size 2x4 to m\n",
    "m * torch.rand(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 26.6596,   5.7720,   6.3961,  49.8958],\n",
       "        [168.8363,   2.1229,   2.0025,  10.4275]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide m by a random tensor of size 2x4\n",
    "m / torch.rand(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.],\n",
       "        [5., 2.],\n",
       "        [3., 1.],\n",
       "        [7., 9.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpose tensor m, which is essentially 2x4 to 4x2\n",
    "m.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.],\n",
       "        [5., 2.],\n",
       "        [3., 1.],\n",
       "        [7., 9.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same as\n",
    "m.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4., 5., 6., 7., 8.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensor from 3 to 8, with each having a space of 1\n",
    "torch.arange(3., 8 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.7000,  2.7000, -0.3000])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensor from 5.7 to -2.1 with each having a space of -3\n",
    "torch.arange(5.7, -2.1, -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.0000, 3.2632, 3.5263, 3.7895, 4.0526, 4.3158, 4.5789, 4.8421, 5.1053,\n",
       "        5.3684, 5.6316, 5.8947, 6.1579, 6.4211, 6.6842, 6.9474, 7.2105, 7.4737,\n",
       "        7.7368, 8.0000])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(3, 8, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0000, 3.2632, 3.5263, 3.7895, 4.0526, 4.3158, 4.5789, 4.8421, 5.1053,\n",
       "         5.3684, 5.6316, 5.8947, 6.1579, 6.4211, 6.6842, 6.9474, 7.2105, 7.4737,\n",
       "         7.7368, 8.0000]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns a 1D tensor of steps equally spaced points between start=3, end=8 and steps=20\n",
    "torch.linspace(3, 8, 20).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor filled with 0's\n",
    "torch.zeros(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor filled with 1's\n",
    "torch.ones(3, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor with the diagonal filled with 1\n",
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default plots\n",
    "from res.plot_lib import set_default\n",
    "from matplotlib import pyplot as plt\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Numpy bridge!\n",
    "plt.hist(torch.randn(1000).numpy(), 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(torch.randn(10**6).numpy(), 100);  # how much does this chart weight?\n",
    "# use rasterized=True for SVG/EPS/PDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(torch.rand(10**6).numpy(), 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the power of GPUs\n",
    "\n",
    "One of PyTorchâ€™s greatest strengths is its ability to leverage GPUs for accelerated computations. However, **by default, PyTorch runs on the CPU**, so we need to explicitly move tensors to the GPU for optimal performance.\n",
    "\n",
    "> PyTorch supports CUDA for NVIDIA GPUs and MPS (Metal Performance Shaders) for Apple Silicon Macs, enabling much faster tensor operations compared to CPUs.\n",
    "\n",
    "**Important Note**: To perform operations between tensors, they must be on the same device (CPU, CUDA, or MPS). Mixing tensors from different devices will result in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Checking for available device (CPU, GPU, MPS)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor on cpu:\n",
      " tensor([[[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor on the CPU (default)\n",
    "z = torch.ones(2, 3, 5)\n",
    "\n",
    "# Move tensor to the device\n",
    "z_1 = z.to(device) # Remember: when moving tensors to a device, you have to reassign them!\n",
    "print(f\"Tensor on {device}:\\n\", z_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple GPUs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor on cpu:\n",
      " tensor([[[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# Multiple GPUs? You can specify which one to use:\n",
    "# e.g., move your tensor to GPU device 0 (first GPU in the system) if there is one\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "z_2 = z.to(device)\n",
    "print(f\"Tensor on {device}:\\n\", z_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not sure which of your GPUs is the first one for PyTorch?\n",
    "gpus = []\n",
    "\n",
    "# List the available GPUs in order\n",
    "if torch.cuda.is_available():\n",
    "    gpus = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n",
    "\n",
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.BFloat16Tensor\n",
       "torch.BoolTensor\n",
       "torch.ByteTensor\n",
       "torch.CharTensor\n",
       "torch.DoubleTensor\n",
       "torch.FloatTensor\n",
       "torch.HalfTensor\n",
       "torch.IntTensor\n",
       "torch.LongTensor\n",
       "torch.ShortTensor\n",
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper to get what kind of tensor types\n",
    "torch.*Tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mm\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is basically a 64 bit float tensor\n",
    "m_double = m.double()\n",
    "m_double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a tensor of type int8\n",
    "m_byte = m.byte()\n",
    "m_byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move your tensor to GPU device 0 if there is one (first GPU in the system)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "m.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts tensor to numpy array\n",
    "m_np = m.numpy()\n",
    "m_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-place fill of column 0 and row 0 with value -1\n",
    "m_np[0, 0] = -1\n",
    "m_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor of integers ranging from 0 to 4\n",
    "import numpy as np\n",
    "n_np = np.arange(5)\n",
    "n = torch.from_numpy(n_np)\n",
    "print(n_np, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-place multiplication of all elements by 2 for tensor n\n",
    "# Because n is essentially n_np, not a clone, this affects n_np\n",
    "n.mul_(2)\n",
    "n_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates two tensors of size 1x4\n",
    "a = torch.Tensor([[1, 2, 3, 4]])\n",
    "b = torch.Tensor([[5, 6, 7, 8]])\n",
    "print(a.size(), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate on axis 0, so you get 2x4\n",
    "torch.cat((a, b), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate on axis 1, so you get 1x8\n",
    "torch.cat((a, b), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Much more\n",
    "\n",
    "There's definitely much more, but this was the basics about `Tensor`s fun.\n",
    "\n",
    "*Torch* full API should be read at least once.\n",
    "Hence, go [here](https://pytorch.org/docs/stable/index.html).\n",
    "You'll find 100+ `Tensor` operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random numbers, etc are described."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
